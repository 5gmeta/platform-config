{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to 5GMETA Project Offical Documentation Requirements MkDocs requires a recent version of Python and the Python package manager, pip , to be installed on your system. Installing MkDocs Install the mkdocs package using pip: pip install mkdocs You should now have the mkdocs command installed on your system. Run mkdocs --version to check that everything worked okay. Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Home"},{"location":"#welcome-to-5gmeta-project-offical-documentation","text":"","title":"Welcome to 5GMETA Project Offical Documentation"},{"location":"#requirements","text":"MkDocs requires a recent version of Python and the Python package manager, pip , to be installed on your system.","title":"Requirements"},{"location":"#installing-mkdocs","text":"Install the mkdocs package using pip: pip install mkdocs You should now have the mkdocs command installed on your system. Run mkdocs --version to check that everything worked okay.","title":"Installing MkDocs"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"cheat-sheet/","text":"CLOUD Platform: 5GMETA Cloud platform has been deployed in AWS EKS service. In the following picture we can see the architecure and the diferent services exposed: The next figure shows the modules that form this cloud platform: Cloud Services shortcuts APIs: Discovery API: https:// /discovery-api/ Instance API: https:// /cloudinstance-api/ Dataflow API: https:// /dataflow-api/ License API: https:// /license-api/ Kafka Platform: Brokers (Bootstrap): :31090 , :31091 , :31092 Registry: :31081 KSQLDB: :31088 Kafka UI: http:// :31080 ( / ) Other services: Keycloak: https:// /identity/admin/ ( / ) Grafana: http:// :31137 ( / ) Prometheus: http:// :30090 Kubernetes Dashboard: https:// :30183 . To get a token run: kubectl -n kubernetes-dashboard get secret $(kubectl -n kubernetes-dashboard get sa/admin-user -o jsonpath=\"{.secrets[0].name}\") -o go-template=\"{{.data.token | base64decode}}\" MySQL: :30057 . To get the root password run: kubectl get secret --namespace mysql mysql-cluster -o jsonpath=\"{.data.mysql-root-password}\" | base64 -d . Another user has been created so that root user is not used in the applications or if the K8s secret is not used: / \ud83d\ude4b All the urls pointing to the public IPs of amazon can be replaces with any other public IP reserved in AWS. For example: MEC Platform: Architecture of the MEC Platform: Modules composing the platform: MEC Services shortcuts APIs: Registration: http:// :12346 Instance API: http:// :5000 (Internal), http:// :5000 (External) Brokers: Message-broker: http:// :8161 (UI), :5673 (Port for producing data, clients), :61616 (Port of ActiveMQ, Kafka Connectors, Internal), :616161 (Port of ActiveMQ, Kafka Connectors, External) Video-broker: :8443 , :<55000-55098> Other services: OSM: http:// OSM API: https:// :9999 Kubernetes Dashboard: https:// :27460 . To get a token run: kubectl -n kubernetes-dashboard get secret $(kubectl -n kubernetes-dashboard get sa/admin-user -o jsonpath=\"{.secrets[0].name}\") -o go-template=\"{{.data.token | base64decode}}\" Grafana: http:// :3000 ( / ) Prometheus: http:// :9090 MySQL: :35430 . To get the root password run: kubectl get secret --namespace mysql mysql-cluster -o jsonpath=\"{.data.mysql-root-password}\" | base64 -d Repositories DockerHub : https://hub.docker.com/orgs/5gmeta/repositories Helm Chart repository : https://github.com/5gmeta/helmcharts OSM descriptors repository : https://github.com/5gmeta/vnfdescriptors","title":"Cheat sheet"},{"location":"cheat-sheet/#cloud-platform","text":"5GMETA Cloud platform has been deployed in AWS EKS service. In the following picture we can see the architecure and the diferent services exposed: The next figure shows the modules that form this cloud platform:","title":"CLOUD Platform:"},{"location":"cheat-sheet/#cloud-services-shortcuts","text":"APIs: Discovery API: https:// /discovery-api/ Instance API: https:// /cloudinstance-api/ Dataflow API: https:// /dataflow-api/ License API: https:// /license-api/ Kafka Platform: Brokers (Bootstrap): :31090 , :31091 , :31092 Registry: :31081 KSQLDB: :31088 Kafka UI: http:// :31080 ( / ) Other services: Keycloak: https:// /identity/admin/ ( / ) Grafana: http:// :31137 ( / ) Prometheus: http:// :30090 Kubernetes Dashboard: https:// :30183 . To get a token run: kubectl -n kubernetes-dashboard get secret $(kubectl -n kubernetes-dashboard get sa/admin-user -o jsonpath=\"{.secrets[0].name}\") -o go-template=\"{{.data.token | base64decode}}\" MySQL: :30057 . To get the root password run: kubectl get secret --namespace mysql mysql-cluster -o jsonpath=\"{.data.mysql-root-password}\" | base64 -d . Another user has been created so that root user is not used in the applications or if the K8s secret is not used: / \ud83d\ude4b All the urls pointing to the public IPs of amazon can be replaces with any other public IP reserved in AWS. For example:","title":"Cloud Services shortcuts"},{"location":"cheat-sheet/#mec-platform","text":"Architecture of the MEC Platform: Modules composing the platform:","title":"MEC Platform:"},{"location":"cheat-sheet/#mec-services-shortcuts","text":"APIs: Registration: http:// :12346 Instance API: http:// :5000 (Internal), http:// :5000 (External) Brokers: Message-broker: http:// :8161 (UI), :5673 (Port for producing data, clients), :61616 (Port of ActiveMQ, Kafka Connectors, Internal), :616161 (Port of ActiveMQ, Kafka Connectors, External) Video-broker: :8443 , :<55000-55098> Other services: OSM: http:// OSM API: https:// :9999 Kubernetes Dashboard: https:// :27460 . To get a token run: kubectl -n kubernetes-dashboard get secret $(kubectl -n kubernetes-dashboard get sa/admin-user -o jsonpath=\"{.secrets[0].name}\") -o go-template=\"{{.data.token | base64decode}}\" Grafana: http:// :3000 ( / ) Prometheus: http:// :9090 MySQL: :35430 . To get the root password run: kubectl get secret --namespace mysql mysql-cluster -o jsonpath=\"{.data.mysql-root-password}\" | base64 -d","title":"MEC Services shortcuts"},{"location":"cheat-sheet/#repositories","text":"DockerHub : https://hub.docker.com/orgs/5gmeta/repositories Helm Chart repository : https://github.com/5gmeta/helmcharts OSM descriptors repository : https://github.com/5gmeta/vnfdescriptors","title":"Repositories"},{"location":"cloud-access-grant/","text":"Kubernetes Cluster Access Grant Adding a new user To add a new user in an already configured EKS cluster: Create the user in AWS EKS Username: user's e-mail address Add it to k8s5gmeta group Create Access keys for the user Go to the user Security credentials tab Create access key Check Command Line Interface Download csv file Save the generated credentiales in AWS Users Create a namespace for the user's company (If it is not already created): kubectl create namespace <COMPANY> Apply the 5gmeta-cluster-access.yaml manifest file to the namespace: kubectl apply -f 5gmeta-cluster-access.yaml -n <COMPANY> . To get all the namespaces that have this role (the namespaces the new user will have access to) run: kubectl get role -A | grep 5gmeta-access-role Send the credentials to the user and inform how to generate the kubeconfig file. Sample mail: ```` Dear XXX, I am sending you the credentials for accessing the cloud platform: https://onetimesecret.com/ You can only open the link once, so save the credentials locally :) You need to install aws cli version2.7.29: * curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64-2.7.29.zip\" -o \"awscliv2.zip\"* * unzip awscliv2.zip * sudo ./aws/install (if you have a previous version installed -> sudo ./aws/install --update and kubectl version 1.26.0. * curl -LO https://dl.k8s.io/release/v1.26.0/bin/linux/amd64/kubectl * sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl Once installed sign-in into AWS with the following command: aws configure. IMPORTANT, select \u201ceu-west-3\u201d region and \"text\" as output format. Then generate the kubeconfig file with the following command: aws eks update-kubeconfig --name 5gmeta-cloud --role-arn arn:aws:iam:: :role/k8s5gmeta After running this command, the kubeconfig file is generated and you will be able to access the cloud cluster with kubectl. You have access to namespace. Please configure it as the default namespace in kubectl with the following command: kubectl config set-context --current --namespace= Please do not hesitate to contact me if you have any doubt. Regards, XXX ```` The steps to generate the kubeconfug file: aws configure aws eks update-kubeconfig --name 5gmeta-cloud --role-arn arn:aws:iam::<idaws>:role/k8s5gmeta kubectl config set-context --current --namespace=<COMPANY> Configuration of the cluster For deploying all the necessary stuff to add users to an EKS hosted K8s cluster, the following tutorial has been followed: https://www.eksworkshop.com/beginner/091_iam-groups/ Run the following commands: aws iam create-group --group-name k8sAdmin aws iam create-group --group-name k8s5gmeta ACCOUNT_ID=<idaws> POLICY=$(echo -n '{\"Version\":\"2012-10-17\",\"Statement\":[{\"Effect\":\"Allow\",\"Principal\":{\"AWS\":\"arn:aws:iam::'; echo -n \"$ACCOUNT_ID\"; echo -n ':root\"},\"Action\":\"sts:AssumeRole\",\"Condition\":{}}]}') echo ACCOUNT_ID=$ACCOUNT_ID echo POLICY=$POLICY aws iam create-role --role-name k8sAdmin --description \"Kubernetes administrator role (for AWS IAM Authenticator for Kubernetes).\" --assume-role-policy-document \"$POLICY\" --output text --query 'Role.Arn' aws iam create-role --role-name k8s5gmeta --description \"5gmeta role for partner access (for AWS IAM Authenticator for Kubernetes).\" --assume-role-policy-document \"$POLICY\" --output text --query 'Role.Arn' ADMIN_GROUP_POLICY=$(echo -n '{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"AllowAssumeOrganizationAccountRole\", \"Effect\": \"Allow\", \"Action\": \"sts:AssumeRole\", \"Resource\": \"arn:aws:iam::'; echo -n \"$ACCOUNT_ID\"; echo -n ':role/k8sAdmin\" }, { \"Sid\": \"AllowEksDescribeCluster\", \"Effect\": \"Allow\", \"Action\": \"eks:DescribeCluster\", \"Resource\": \"arn:aws:eks:eu-west-3:<idaws>:cluster/5gmeta-cloud\" } ] }') echo ADMIN_GROUP_POLICY=$ADMIN_GROUP_POLICY aws iam put-group-policy \\ --group-name k8sAdmin \\ --policy-name k8sAdmin-policy \\ --policy-document \"$ADMIN_GROUP_POLICY\" 5GMETA_GROUP_POLICY=$(echo -n '{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"AllowAssumeOrganizationAccountRole\", \"Effect\": \"Allow\", \"Action\": \"sts:AssumeRole\", \"Resource\": \"arn:aws:iam::'; echo -n \"$ACCOUNT_ID\"; echo -n ':role/k8s5gmeta\" }, { \"Sid\": \"AllowEksDescribeCluster\", \"Effect\": \"Allow\", \"Action\": \"eks:DescribeCluster\", \"Resource\": \"arn:aws:eks:eu-west-3:<idaws>:cluster/5gmeta-cloud\" } ] }') echo 5GMETA_GROUP_POLICY=$5GMETA_GROUP_POLICY aws iam put-group-policy --group-name k8s5gmeta --policy-name k8s5gmeta-policy --policy-document \"$5GMETA_GROUP_POLICY\" aws iam list-groups eksctl create iamidentitymapping --cluster 5gmeta-cloud --arn arn:aws:iam::${ACCOUNT_ID}:role/k8sAdmin --username admin --group system:masters eksctl create iamidentitymapping --cluster 5gmeta-cloud --arn arn:aws:iam::${ACCOUNT_ID}:role/k8s5gmeta --username <username> kubectl get cm -n kube-system aws-auth -o yaml eksctl get iamidentitymapping --cluster 5gmeta-cloud Create 5gmeta-cluster-access.yaml manifest file and apply it to namespaces that will access k8s5gmeta group users: --- kind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: name: 5gmeta-access-role rules: - apiGroups: - \"\" - \"apps\" - \"batch\" - \"extensions\" - \"autoscaling\" resources: [\"*\"] # resources: # - \"configmaps\" # - \"cronjobs\" # - \"deployments\" # - \"events\" # - \"ingresses\" # - \"jobs\" # - \"pods\" # - \"pods/attach\" # - \"pods/exec\" # - \"pods/log\" # - \"pods/portforward\" # - \"secrets\" # - \"services\" verbs: [\"*\"] # verbs: # - \"create\" # - \"delete\" # - \"describe\" # - \"get\" # - \"list\" # - \"patch\" # - \"update\" --- kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: 5gmeta-role-binding subjects: - kind: User name: <username> roleRef: kind: Role name: 5gmeta-access-role apiGroup: rbac.authorization.k8s.io To get all the namespaces that have this role (the namespaces the new user will have access to) run: kubectl get role -A | grep 5gmeta-access-role","title":"Kubernetes Cluster Access Grant"},{"location":"cloud-access-grant/#kubernetes-cluster-access-grant","text":"","title":"Kubernetes Cluster Access Grant"},{"location":"cloud-access-grant/#adding-a-new-user","text":"To add a new user in an already configured EKS cluster: Create the user in AWS EKS Username: user's e-mail address Add it to k8s5gmeta group Create Access keys for the user Go to the user Security credentials tab Create access key Check Command Line Interface Download csv file Save the generated credentiales in AWS Users Create a namespace for the user's company (If it is not already created): kubectl create namespace <COMPANY> Apply the 5gmeta-cluster-access.yaml manifest file to the namespace: kubectl apply -f 5gmeta-cluster-access.yaml -n <COMPANY> . To get all the namespaces that have this role (the namespaces the new user will have access to) run: kubectl get role -A | grep 5gmeta-access-role Send the credentials to the user and inform how to generate the kubeconfig file. Sample mail: ```` Dear XXX, I am sending you the credentials for accessing the cloud platform: https://onetimesecret.com/ You can only open the link once, so save the credentials locally :) You need to install aws cli version2.7.29: * curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64-2.7.29.zip\" -o \"awscliv2.zip\"* * unzip awscliv2.zip * sudo ./aws/install (if you have a previous version installed -> sudo ./aws/install --update and kubectl version 1.26.0. * curl -LO https://dl.k8s.io/release/v1.26.0/bin/linux/amd64/kubectl * sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl Once installed sign-in into AWS with the following command: aws configure. IMPORTANT, select \u201ceu-west-3\u201d region and \"text\" as output format. Then generate the kubeconfig file with the following command: aws eks update-kubeconfig --name 5gmeta-cloud --role-arn arn:aws:iam:: :role/k8s5gmeta After running this command, the kubeconfig file is generated and you will be able to access the cloud cluster with kubectl. You have access to namespace. Please configure it as the default namespace in kubectl with the following command: kubectl config set-context --current --namespace= Please do not hesitate to contact me if you have any doubt. Regards, XXX ```` The steps to generate the kubeconfug file: aws configure aws eks update-kubeconfig --name 5gmeta-cloud --role-arn arn:aws:iam::<idaws>:role/k8s5gmeta kubectl config set-context --current --namespace=<COMPANY>","title":"Adding a new user"},{"location":"cloud-access-grant/#configuration-of-the-cluster","text":"For deploying all the necessary stuff to add users to an EKS hosted K8s cluster, the following tutorial has been followed: https://www.eksworkshop.com/beginner/091_iam-groups/ Run the following commands: aws iam create-group --group-name k8sAdmin aws iam create-group --group-name k8s5gmeta ACCOUNT_ID=<idaws> POLICY=$(echo -n '{\"Version\":\"2012-10-17\",\"Statement\":[{\"Effect\":\"Allow\",\"Principal\":{\"AWS\":\"arn:aws:iam::'; echo -n \"$ACCOUNT_ID\"; echo -n ':root\"},\"Action\":\"sts:AssumeRole\",\"Condition\":{}}]}') echo ACCOUNT_ID=$ACCOUNT_ID echo POLICY=$POLICY aws iam create-role --role-name k8sAdmin --description \"Kubernetes administrator role (for AWS IAM Authenticator for Kubernetes).\" --assume-role-policy-document \"$POLICY\" --output text --query 'Role.Arn' aws iam create-role --role-name k8s5gmeta --description \"5gmeta role for partner access (for AWS IAM Authenticator for Kubernetes).\" --assume-role-policy-document \"$POLICY\" --output text --query 'Role.Arn' ADMIN_GROUP_POLICY=$(echo -n '{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"AllowAssumeOrganizationAccountRole\", \"Effect\": \"Allow\", \"Action\": \"sts:AssumeRole\", \"Resource\": \"arn:aws:iam::'; echo -n \"$ACCOUNT_ID\"; echo -n ':role/k8sAdmin\" }, { \"Sid\": \"AllowEksDescribeCluster\", \"Effect\": \"Allow\", \"Action\": \"eks:DescribeCluster\", \"Resource\": \"arn:aws:eks:eu-west-3:<idaws>:cluster/5gmeta-cloud\" } ] }') echo ADMIN_GROUP_POLICY=$ADMIN_GROUP_POLICY aws iam put-group-policy \\ --group-name k8sAdmin \\ --policy-name k8sAdmin-policy \\ --policy-document \"$ADMIN_GROUP_POLICY\" 5GMETA_GROUP_POLICY=$(echo -n '{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"AllowAssumeOrganizationAccountRole\", \"Effect\": \"Allow\", \"Action\": \"sts:AssumeRole\", \"Resource\": \"arn:aws:iam::'; echo -n \"$ACCOUNT_ID\"; echo -n ':role/k8s5gmeta\" }, { \"Sid\": \"AllowEksDescribeCluster\", \"Effect\": \"Allow\", \"Action\": \"eks:DescribeCluster\", \"Resource\": \"arn:aws:eks:eu-west-3:<idaws>:cluster/5gmeta-cloud\" } ] }') echo 5GMETA_GROUP_POLICY=$5GMETA_GROUP_POLICY aws iam put-group-policy --group-name k8s5gmeta --policy-name k8s5gmeta-policy --policy-document \"$5GMETA_GROUP_POLICY\" aws iam list-groups eksctl create iamidentitymapping --cluster 5gmeta-cloud --arn arn:aws:iam::${ACCOUNT_ID}:role/k8sAdmin --username admin --group system:masters eksctl create iamidentitymapping --cluster 5gmeta-cloud --arn arn:aws:iam::${ACCOUNT_ID}:role/k8s5gmeta --username <username> kubectl get cm -n kube-system aws-auth -o yaml eksctl get iamidentitymapping --cluster 5gmeta-cloud Create 5gmeta-cluster-access.yaml manifest file and apply it to namespaces that will access k8s5gmeta group users: --- kind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: name: 5gmeta-access-role rules: - apiGroups: - \"\" - \"apps\" - \"batch\" - \"extensions\" - \"autoscaling\" resources: [\"*\"] # resources: # - \"configmaps\" # - \"cronjobs\" # - \"deployments\" # - \"events\" # - \"ingresses\" # - \"jobs\" # - \"pods\" # - \"pods/attach\" # - \"pods/exec\" # - \"pods/log\" # - \"pods/portforward\" # - \"secrets\" # - \"services\" verbs: [\"*\"] # verbs: # - \"create\" # - \"delete\" # - \"describe\" # - \"get\" # - \"list\" # - \"patch\" # - \"update\" --- kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: 5gmeta-role-binding subjects: - kind: User name: <username> roleRef: kind: Role name: 5gmeta-access-role apiGroup: rbac.authorization.k8s.io To get all the namespaces that have this role (the namespaces the new user will have access to) run: kubectl get role -A | grep 5gmeta-access-role","title":"Configuration of the cluster"},{"location":"cloud-database/","text":"Cloud Database: MySQL MySQL is a fast, reliable, scalable, and easy to use open source relational database system. Designed to handle mission-critical, heavy-load production applications. The charm used bootstraps a MySQL replication cluster deployment on a Kubernetes cluster using the Helm package manager. For installing it: helm repo add bitnami [https://charts.bitnami.com/bitnami](https://charts.bitnami.com/bitnami) helm repo update helm install mysql-cluster -n mysql --create-namespace bitnami/mysql --set metrics.enabled=true --set metrics.serviceMonitor.enabled=true --set metrics.serviceMonitor.namespace=monitoring --set metrics.serviceMonitor.labels.release=prometheus-stack --set primary.service.type=NodePort --set primary.service.nodePorts.mysql=30057 Apply https://github.com/5gmeta/dataflow_cloud/blob/main/src/mysql/dataflow_DB_CLOUD_mysql.sql script to a new database called dataflowdb. Apply https://github.com/5gmeta/discovery/blob/main/src/data/sql/database.sql to a new database called discoverydb. Create user in the database: 5gmeta-platform grant access for all the databases When a new mysql cluster has been deployed, it has some secrets that has to be shared with rest of the APIS, so, in all namespaces that run an API that needs access to the mysql database in the cloud: Remove previous secret from mysql in the namespace where API is running kubectl delete secret mysql-cluster -n Copy secret from mysql cluster to namespaces that run APIs with access to the database: kubectl get secret mysql-cluster --namespace=mysql -o yaml | sed 's/namespace: .*/namespace: /' | kubectl apply -f - MySQL Grafana dashboard To be imported in Grafana, https://grafana.com/grafana/dashboards/7362","title":"Cloud Database: MySQL"},{"location":"cloud-database/#cloud-database-mysql","text":"MySQL is a fast, reliable, scalable, and easy to use open source relational database system. Designed to handle mission-critical, heavy-load production applications. The charm used bootstraps a MySQL replication cluster deployment on a Kubernetes cluster using the Helm package manager. For installing it: helm repo add bitnami [https://charts.bitnami.com/bitnami](https://charts.bitnami.com/bitnami) helm repo update helm install mysql-cluster -n mysql --create-namespace bitnami/mysql --set metrics.enabled=true --set metrics.serviceMonitor.enabled=true --set metrics.serviceMonitor.namespace=monitoring --set metrics.serviceMonitor.labels.release=prometheus-stack --set primary.service.type=NodePort --set primary.service.nodePorts.mysql=30057 Apply https://github.com/5gmeta/dataflow_cloud/blob/main/src/mysql/dataflow_DB_CLOUD_mysql.sql script to a new database called dataflowdb. Apply https://github.com/5gmeta/discovery/blob/main/src/data/sql/database.sql to a new database called discoverydb. Create user in the database: 5gmeta-platform grant access for all the databases When a new mysql cluster has been deployed, it has some secrets that has to be shared with rest of the APIS, so, in all namespaces that run an API that needs access to the mysql database in the cloud: Remove previous secret from mysql in the namespace where API is running kubectl delete secret mysql-cluster -n Copy secret from mysql cluster to namespaces that run APIs with access to the database: kubectl get secret mysql-cluster --namespace=mysql -o yaml | sed 's/namespace: .*/namespace: /' | kubectl apply -f - MySQL Grafana dashboard To be imported in Grafana, https://grafana.com/grafana/dashboards/7362","title":"Cloud Database: MySQL"},{"location":"cloud-deployment/","text":"Cloud platform deployment For deploying 5GMETA Cloud Platform, AWS EKS solution has been chosen. Amazon Elastic Kubernetes Service ( Amazon EKS ) is a managed service that you can use to run Kubernetes on AWS without needing to install, operate, and maintain your own Kubernetes control plane or nodes. Kubernetes is an open-source system for automating the deployment, scaling, and management of containerized applications. Amazon EKS runs a single tenant Kubernetes control plane for each cluster. The control plane infrastructure isn't shared across clusters or AWS accounts. The control plane consists of at least two API server instances and three etcd instances that run across three Availability Zones within an AWS Region. The control plane runs in an account managed by AWS, and the Kubernetes API is exposed via the Amazon EKS endpoint associated with your cluster. Each Amazon EKS cluster control plane is single-tenant and unique, and runs on its own set of Amazon EC2 instances. The workloads are deployed in Amazon EKS nodes that are registered with the control plane. They are deployed in EC2 service. The following picture shows the architecurte of an AWS EKS cluster: Prerequisites Many procedures of the guide use the following command line tools: kubectl \u2013 A command line tool for working with Kubernetes clusters. For more information, see Installing or updating kubectl . ```` curl -LO https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl curl -LO https://dl.k8s.io/release/v1.21.0/bin/linux/amd64/kubectl sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl kubectl version --client - `eksctl` \u2013 A command line tool for working with EKS clusters that automates many individual tasks. For more information, see [Installing or updating eksctl](https://docs.aws.amazon.com/eks/latest/userguide/eksctl.html). curl --silent --location \"https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz\" | tar xz -C /tmp sudo mv /tmp/eksctl /usr/local/bin eksctl version ```` AWS CLI \u2013 A command line tool for working with AWS services, including Amazon EKS. For more information, see Installing, updating, and uninstalling the AWS CLI in the AWS Command Line Interface User Guide. After installing the AWS CLI, we recommend that you also configure it. For more information, see Quick configuration with aws configure in the AWS Command Line Interface User Guide. ```` curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\" unzip awscliv2.zip sudo ./aws/install aws \u2013version aws configure - `helm` \u2013 Helm helps you manage Kubernetes applications \u2014 Helm Charts help you define, install, and upgrade even the most complex Kubernetes application. For more information, see [Installing Helm](https://helm.sh/docs/intro/install/). curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 chmod 700 get_helm.sh ./get_helm.sh helm version ```` Step 1: Create Amazon EKS cluster and nodes Before creating a cluster and nodes for production use, familiarize yourself with all settings and deploy a cluster and nodes with the settings that meet your requirements. For more information, see Creating an Amazon EKS cluster and Amazon EKS nodes . Some settings can only be enabled when creating your cluster and nodes. Create your Amazon EKS cluster with the following command. eksctl create cluster -f 5gmeta-cluster-config.yaml eksctl get cluster aws eks describe-cluster --name 5gmeta-cloud helm repo add 5gmeta https://<id>@raw.githubusercontent.com/5gmetadmin/helmcharts/main/repository kubectl create secret docker-registry regcred --docker-server=https://index.docker.io/v1/ --docker-username=<user> --docker-password=<password> The yaml configuration file can be found in this repository: 5gmeta-cluster-config.yaml The cluster that will be created will be made up of two nodes, and can be scaled up to 4 nodes without the cluster running out of resources. The nodes will be deployed in the eu-west-3 region, with Kubernetes version 1.21 and with the instance type t3a.large (2 vCPU, 8GiB Memory). 5gmeta-cluster-ssh-key.pem key is used for ssh access. It was previously created using EC2 AWS interface . To avoid unneeded costs when nodes are going to be in public nodegroups anyway, default NAT gateway creation is disabled. Automatic creation of IAM Roles for autoScaler, ebs and albIngress are enabled. apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: 5gmeta-cloud region: eu-west-3 version: \"1.21\" vpc: nat: gateway: Disable # other options: HighlyAvailable, Single iam: withOIDC: true managedNodeGroups: - name: managed-ng-1 instanceType: t3a.large desiredCapacity: 2 minSize: 2 maxSize: 4 volumeSize: 50 ssh: allow: true # will use ~/.ssh/id_rsa.pub as the default ssh key if no publicKeyNamem specified publicKeyName: 5gmeta-cluster-key # key created in aws iam: withAddonPolicies: autoScaler: true ebs: true albIngress: true ami: ami-<id> # get ami id 'aws ssm get-parameter --name /aws/service/eks/optimized-ami/1.21/amazon-linux-2/recommended/image_id --region eu-west-3 --query \"Parameter.Value\" --output text' overrideBootstrapCommand: | #!/bin/bash /etc/eks/bootstrap.sh 5gmeta-cloud --kubelet-extra-args \"--kube-reserved memory=0.3Gi,ephemeral-storage=1Gi --system-reserved memory=0.3Gi,ephemeral-storage=1Gi --eviction-hard memory.available<200Mi,nodefs.available<10%\" Step 2: Configure Amazon EKS cluster Create an IAM OIDC provider: Amazon EKS supports using OpenID Connect (OIDC) identity providers as a method to authenticate users in the cluster. After configuring authentication in the cluster, Kubernetes roles and clusterroles can be created to assign permissions to the roles, and then bind the roles to the identities using Kubernetes rolebindings and clusterrolebindings . The cluster has an OpenID Connect (OIDC) issuer URL associated with it. To use AWS Identity and Access Management (IAM) roles for service accounts, an IAM OIDC provider must exist. oidc_id=$(aws eks describe-cluster --name 5gmeta-cloud --query \"cluster.identity.oidc.issuer\" --output text | cut -d '/' -f 5) aws iam list-open-id-connect-providers | grep $oidc_id If output is returned from the previous command, a provider for the cluster is already created and next step can be skipped. If no output is returned, then an IAM OIDC provider must be created. eksctl utils associate-iam-oidc-provider --cluster 5gmeta-cloud --approve Install AWS Load Balancer Controller add-on: The AWS Load Balancer Controller manages AWS Elastic Load Balancers for a Kubernetes cluster. The controller provisions the following resources. - An AWS Application Load Balancer (ALB) when you create a Kubernetes Ingress. - An AWS Network Load Balancer (NLB) when you create a Kubernetes service of type LoadBalancer. In the past, the Kubernetes network load balancer was used for instance targets, but the AWS Load balancer Controller was used for IP targets. With the AWS Load Balancer Controller version 2.3.0 or later, you can create NLBs using either target type. For more information about NLB target types, see Target type in the User Guide for Network Load Balancers. The AWS Load Balancer Controller was formerly named the AWS ALB Ingress Controller . It's an open-source project managed on GitHub. This topic describes how to install the controller using default options. You can view the full documentation for the controller on GitHub. Before deploying the controller, we recommend that you review the prerequisites and considerations in Application load balancing on Amazon EKS and Network load balancing on Amazon EKS . Those topics also include steps on how to deploy a sample application that require the AWS Load Balancer Controller to provision AWS Application Load Balancers and Network Load Balancers. To deploy the AWS Load Balancer Controller to an Amazon EKS cluster: curl -o iam_policy.json https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.4.1/docs/install/iam_policy.json aws iam create-policy --policy-name AWSLoadBalancerControllerIAMPolicy --policy-document file://iam_policy.json eksctl create iamserviceaccount --cluster=5gmeta-cloud --namespace=kube-system --name=aws-load-balancer-controller --attach-policy-arn=arn:aws:iam::<idaws>:policy/AWSLoadBalancerControllerIAMPolicy --override-existing-serviceaccounts --approve #<idaws> refers to the account ID helm repo add eks https://aws.github.io/eks-charts helm repo update helm install aws-load-balancer-controller eks/aws-load-balancer-controller -n kube-system --set clusterName=5gmeta-cloud --set serviceAccount.create=false --set serviceAccount.name=aws-load-balancer-controller kubectl get deployment -n kube-system aws-load-balancer-controller Install AWS Storage Controller Amazon EKS clusters that were created prior to Kubernetes version 1.11 weren't created with any storage classes. You must define storage classes for your cluster to use and you should define a default storage class for your persistent volume claims. For more information, see Storage classes in the Kubernetes documentation. The Amazon Elastic Block Store (Amazon EBS) Container Storage Interface (CSI) driver allows Amazon Elastic Kubernetes Service (Amazon EKS) clusters to manage the lifecycle of Amazon EBS volumes for persistent volumes. The Amazon EBS CSI driver isn't installed when you first create a cluster. More information about ebs.csi.aws.com and kubernetes.io/aws-ebs provisioners: https://stackoverflow.com/questions/68359043/whats-the-difference-between-ebs-csi-aws-com-vs-kubernetes-io-aws-ebs-for-provi curl -o example-iam-policy.json https://raw.githubusercontent.com/kubernetes-sigs/aws-ebs-csi-driver/master/docs/example-iam-policy.json aws iam create-policy --policy-name AmazonEKS_EBS_CSI_Driver_Policy --policy-document file://example-iam-policy.json eksctl create iamserviceaccount --name ebs-csi-controller-sa --namespace kube-system --cluster 5gmeta-cloud --attach-policy-arn arn:aws:iam::<idaws>:policy/AmazonEKS_EBS_CSI_Driver_Policy --approve --override-existing-serviceaccounts #<idaws> refers to the account ID helm repo add aws-ebs-csi-driver https://kubernetes-sigs.github.io/aws-ebs-csi-driver helm repo update helm upgrade -install aws-ebs-csi-driver aws-ebs-csi-driver/aws-ebs-csi-driver --namespace kube-system --set controller.serviceAccount.create=false --set controller.serviceAccount.name=ebs-csi-controller-sa kubectl get deployment -n kube-system ebs-csi-controller kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/aws-ebs-csi-driver/master/examples/kubernetes/dynamic-provisioning/manifests/storageclass.yaml kubectl get storageclass kubectl patch storageclass gp2 -p '{\"metadata\":{\"annotations\":{\"storageclass.kubernetes.io/is-default-class\":\"false\"}}}' kubectl patch storageclass ebs-sc -p '{\"metadata\":{\"annotations\":{\"storageclass.kubernetes.io/is-default-class\":\"true\"}}}' kubectl get storageclass Define the default storage class A StorageClass provides a way for administrators to describe the \"classes\" of storage they offer. Dynamic Volume Provisioning allows storage volumes and persistent volumes to be created on-demand. StorageClass should be pre-created to define which provisioner should be used and what parameters should be passed when dynamic provisioning is invoked. To define the storage class the following manifest file will be applied. Thanks to this storage class every persistent volume will be deployed in AWS S3, using the previously installed storage controller. apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: ebs-sc-retain provisioner: ebs.csi.aws.com reclaimPolicy: Retain volumeBindingMode: WaitForFirstConsumer To become the storage class the default one: kubectl annotate storageclass gp2 storageclass.kubernetes.io/is-default-class=true Install Cluster Autoscaler The Kubernetes Cluster Autoscaler automatically adjusts the number of nodes in your cluster when pods fail or are rescheduled onto other nodes. The Cluster Autoscaler is typically installed as a Deployment in your cluster. It uses leader election to ensure high availability, but scaling is done by only one replica at a time. Create an IAM policy that grants the permissions that the Cluster Autoscaler requires to use an IAM role. Save the following contents to a file that's named cluster-autoscaler-policy.json . { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"autoscaling:DescribeAutoScalingGroups\", \"autoscaling:DescribeAutoScalingInstances\", \"autoscaling:DescribeLaunchConfigurations\", \"autoscaling:DescribeTags\", \"ec2:DescribeInstanceTypes\", \"ec2:DescribeLaunchTemplateVersions\" ], \"Resource\": [\"*\"] }, { \"Effect\": \"Allow\", \"Action\": [ \"autoscaling:SetDesiredCapacity\", \"autoscaling:TerminateInstanceInAutoScalingGroup\" ], \"Resource\": [\"*\"] } ] } Create the policy with the following command. aws iam create-policy --policy-name AmazonEKSClusterAutoscalerPolicy --policy-document file://cluster-autoscaler-policy.json eksctl create iamserviceaccount --cluster=5gmeta-cloud --namespace=kube-system --name=cluster-autoscaler --attach-policy-arn=arn:aws:iam::<idaws>:policy/AmazonEKSClusterAutoscalerPolicy --override-existing-serviceaccounts --approve #<idaws> refers to the account ID Complete the following steps to deploy the Cluster Autoscaler. We recommend that you review Deployment considerations and optimize the Cluster Autoscaler deployment before you deploy it to a production cluster. curl -o cluster-autoscaler-autodiscover.yaml https://raw.githubusercontent.com/kubernetes/autoscaler/master/cluster-autoscaler/cloudprovider/aws/examples/cluster-autoscaler-autodiscover.yaml Edit the cluster-autoscaler manifest to replace (including <>) with the name of your cluster, and add the following options. --balance-similar-node-groups ensures that there is enough available compute across all availability zones. --skip-nodes-with-system-pods=false ensures that there are no problems with scaling to zero. kubectl apply -f cluster-autoscaler-autodiscover.yaml kubectl annotate serviceaccount cluster-autoscaler -n kube-system eks.amazonaws.com/role-arn=arn:aws:iam::<idaws>:role/AmazonEKSClusterAutoscalerRole --aprove #<idaws> refers to the account ID kubectl patch deployment cluster-autoscaler -n kube-system -p '{\"spec\":{\"template\":{\"metadata\":{\"annotations\":{\"cluster-autoscaler.kubernetes.io/safe-to-evict\": \"false\"}}}}}' export K8S_VERSION=$(kubectl version --short | grep 'Server Version:' | sed 's/[^0-9.]*\\([0-9.]*\\).*/\\1/' | cut -d. -f1,2) export AUTOSCALER_VERSION=$(curl -s \"https://api.github.com/repos/kubernetes/autoscaler/releases\" | grep '\"tag_name\":' | sed -s 's/.*-\\([0-9][0-9\\.]*\\).*/\\1/' | grep -m1 ${K8S_VERSION}) kubectl set image deployment cluster-autoscaler -n kube-system cluster-autoscaler=k8s.gcr.io/autoscaling/cluster-autoscaler:v${AUTOSCALER_VERSION} kubectl -n kube-system logs -f deployment.apps/cluster-autoscaler Modify AWS Security Group for accessing NodePorts (30000-32768) from outside (if needed)","title":"Cloud platform deployment"},{"location":"cloud-deployment/#cloud-platform-deployment","text":"For deploying 5GMETA Cloud Platform, AWS EKS solution has been chosen. Amazon Elastic Kubernetes Service ( Amazon EKS ) is a managed service that you can use to run Kubernetes on AWS without needing to install, operate, and maintain your own Kubernetes control plane or nodes. Kubernetes is an open-source system for automating the deployment, scaling, and management of containerized applications. Amazon EKS runs a single tenant Kubernetes control plane for each cluster. The control plane infrastructure isn't shared across clusters or AWS accounts. The control plane consists of at least two API server instances and three etcd instances that run across three Availability Zones within an AWS Region. The control plane runs in an account managed by AWS, and the Kubernetes API is exposed via the Amazon EKS endpoint associated with your cluster. Each Amazon EKS cluster control plane is single-tenant and unique, and runs on its own set of Amazon EC2 instances. The workloads are deployed in Amazon EKS nodes that are registered with the control plane. They are deployed in EC2 service. The following picture shows the architecurte of an AWS EKS cluster:","title":"Cloud platform deployment"},{"location":"cloud-deployment/#prerequisites","text":"Many procedures of the guide use the following command line tools: kubectl \u2013 A command line tool for working with Kubernetes clusters. For more information, see Installing or updating kubectl . ```` curl -LO https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl curl -LO https://dl.k8s.io/release/v1.21.0/bin/linux/amd64/kubectl sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl kubectl version --client - `eksctl` \u2013 A command line tool for working with EKS clusters that automates many individual tasks. For more information, see [Installing or updating eksctl](https://docs.aws.amazon.com/eks/latest/userguide/eksctl.html). curl --silent --location \"https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz\" | tar xz -C /tmp sudo mv /tmp/eksctl /usr/local/bin eksctl version ```` AWS CLI \u2013 A command line tool for working with AWS services, including Amazon EKS. For more information, see Installing, updating, and uninstalling the AWS CLI in the AWS Command Line Interface User Guide. After installing the AWS CLI, we recommend that you also configure it. For more information, see Quick configuration with aws configure in the AWS Command Line Interface User Guide. ```` curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\" unzip awscliv2.zip sudo ./aws/install aws \u2013version aws configure - `helm` \u2013 Helm helps you manage Kubernetes applications \u2014 Helm Charts help you define, install, and upgrade even the most complex Kubernetes application. For more information, see [Installing Helm](https://helm.sh/docs/intro/install/). curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 chmod 700 get_helm.sh ./get_helm.sh helm version ````","title":"Prerequisites"},{"location":"cloud-deployment/#step-1-create-amazon-eks-cluster-and-nodes","text":"Before creating a cluster and nodes for production use, familiarize yourself with all settings and deploy a cluster and nodes with the settings that meet your requirements. For more information, see Creating an Amazon EKS cluster and Amazon EKS nodes . Some settings can only be enabled when creating your cluster and nodes. Create your Amazon EKS cluster with the following command. eksctl create cluster -f 5gmeta-cluster-config.yaml eksctl get cluster aws eks describe-cluster --name 5gmeta-cloud helm repo add 5gmeta https://<id>@raw.githubusercontent.com/5gmetadmin/helmcharts/main/repository kubectl create secret docker-registry regcred --docker-server=https://index.docker.io/v1/ --docker-username=<user> --docker-password=<password> The yaml configuration file can be found in this repository: 5gmeta-cluster-config.yaml The cluster that will be created will be made up of two nodes, and can be scaled up to 4 nodes without the cluster running out of resources. The nodes will be deployed in the eu-west-3 region, with Kubernetes version 1.21 and with the instance type t3a.large (2 vCPU, 8GiB Memory). 5gmeta-cluster-ssh-key.pem key is used for ssh access. It was previously created using EC2 AWS interface . To avoid unneeded costs when nodes are going to be in public nodegroups anyway, default NAT gateway creation is disabled. Automatic creation of IAM Roles for autoScaler, ebs and albIngress are enabled. apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: 5gmeta-cloud region: eu-west-3 version: \"1.21\" vpc: nat: gateway: Disable # other options: HighlyAvailable, Single iam: withOIDC: true managedNodeGroups: - name: managed-ng-1 instanceType: t3a.large desiredCapacity: 2 minSize: 2 maxSize: 4 volumeSize: 50 ssh: allow: true # will use ~/.ssh/id_rsa.pub as the default ssh key if no publicKeyNamem specified publicKeyName: 5gmeta-cluster-key # key created in aws iam: withAddonPolicies: autoScaler: true ebs: true albIngress: true ami: ami-<id> # get ami id 'aws ssm get-parameter --name /aws/service/eks/optimized-ami/1.21/amazon-linux-2/recommended/image_id --region eu-west-3 --query \"Parameter.Value\" --output text' overrideBootstrapCommand: | #!/bin/bash /etc/eks/bootstrap.sh 5gmeta-cloud --kubelet-extra-args \"--kube-reserved memory=0.3Gi,ephemeral-storage=1Gi --system-reserved memory=0.3Gi,ephemeral-storage=1Gi --eviction-hard memory.available<200Mi,nodefs.available<10%\"","title":"Step 1: Create Amazon EKS cluster and nodes"},{"location":"cloud-deployment/#step-2-configure-amazon-eks-cluster","text":"Create an IAM OIDC provider: Amazon EKS supports using OpenID Connect (OIDC) identity providers as a method to authenticate users in the cluster. After configuring authentication in the cluster, Kubernetes roles and clusterroles can be created to assign permissions to the roles, and then bind the roles to the identities using Kubernetes rolebindings and clusterrolebindings . The cluster has an OpenID Connect (OIDC) issuer URL associated with it. To use AWS Identity and Access Management (IAM) roles for service accounts, an IAM OIDC provider must exist. oidc_id=$(aws eks describe-cluster --name 5gmeta-cloud --query \"cluster.identity.oidc.issuer\" --output text | cut -d '/' -f 5) aws iam list-open-id-connect-providers | grep $oidc_id If output is returned from the previous command, a provider for the cluster is already created and next step can be skipped. If no output is returned, then an IAM OIDC provider must be created. eksctl utils associate-iam-oidc-provider --cluster 5gmeta-cloud --approve Install AWS Load Balancer Controller add-on: The AWS Load Balancer Controller manages AWS Elastic Load Balancers for a Kubernetes cluster. The controller provisions the following resources. - An AWS Application Load Balancer (ALB) when you create a Kubernetes Ingress. - An AWS Network Load Balancer (NLB) when you create a Kubernetes service of type LoadBalancer. In the past, the Kubernetes network load balancer was used for instance targets, but the AWS Load balancer Controller was used for IP targets. With the AWS Load Balancer Controller version 2.3.0 or later, you can create NLBs using either target type. For more information about NLB target types, see Target type in the User Guide for Network Load Balancers. The AWS Load Balancer Controller was formerly named the AWS ALB Ingress Controller . It's an open-source project managed on GitHub. This topic describes how to install the controller using default options. You can view the full documentation for the controller on GitHub. Before deploying the controller, we recommend that you review the prerequisites and considerations in Application load balancing on Amazon EKS and Network load balancing on Amazon EKS . Those topics also include steps on how to deploy a sample application that require the AWS Load Balancer Controller to provision AWS Application Load Balancers and Network Load Balancers. To deploy the AWS Load Balancer Controller to an Amazon EKS cluster: curl -o iam_policy.json https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.4.1/docs/install/iam_policy.json aws iam create-policy --policy-name AWSLoadBalancerControllerIAMPolicy --policy-document file://iam_policy.json eksctl create iamserviceaccount --cluster=5gmeta-cloud --namespace=kube-system --name=aws-load-balancer-controller --attach-policy-arn=arn:aws:iam::<idaws>:policy/AWSLoadBalancerControllerIAMPolicy --override-existing-serviceaccounts --approve #<idaws> refers to the account ID helm repo add eks https://aws.github.io/eks-charts helm repo update helm install aws-load-balancer-controller eks/aws-load-balancer-controller -n kube-system --set clusterName=5gmeta-cloud --set serviceAccount.create=false --set serviceAccount.name=aws-load-balancer-controller kubectl get deployment -n kube-system aws-load-balancer-controller Install AWS Storage Controller Amazon EKS clusters that were created prior to Kubernetes version 1.11 weren't created with any storage classes. You must define storage classes for your cluster to use and you should define a default storage class for your persistent volume claims. For more information, see Storage classes in the Kubernetes documentation. The Amazon Elastic Block Store (Amazon EBS) Container Storage Interface (CSI) driver allows Amazon Elastic Kubernetes Service (Amazon EKS) clusters to manage the lifecycle of Amazon EBS volumes for persistent volumes. The Amazon EBS CSI driver isn't installed when you first create a cluster. More information about ebs.csi.aws.com and kubernetes.io/aws-ebs provisioners: https://stackoverflow.com/questions/68359043/whats-the-difference-between-ebs-csi-aws-com-vs-kubernetes-io-aws-ebs-for-provi curl -o example-iam-policy.json https://raw.githubusercontent.com/kubernetes-sigs/aws-ebs-csi-driver/master/docs/example-iam-policy.json aws iam create-policy --policy-name AmazonEKS_EBS_CSI_Driver_Policy --policy-document file://example-iam-policy.json eksctl create iamserviceaccount --name ebs-csi-controller-sa --namespace kube-system --cluster 5gmeta-cloud --attach-policy-arn arn:aws:iam::<idaws>:policy/AmazonEKS_EBS_CSI_Driver_Policy --approve --override-existing-serviceaccounts #<idaws> refers to the account ID helm repo add aws-ebs-csi-driver https://kubernetes-sigs.github.io/aws-ebs-csi-driver helm repo update helm upgrade -install aws-ebs-csi-driver aws-ebs-csi-driver/aws-ebs-csi-driver --namespace kube-system --set controller.serviceAccount.create=false --set controller.serviceAccount.name=ebs-csi-controller-sa kubectl get deployment -n kube-system ebs-csi-controller kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/aws-ebs-csi-driver/master/examples/kubernetes/dynamic-provisioning/manifests/storageclass.yaml kubectl get storageclass kubectl patch storageclass gp2 -p '{\"metadata\":{\"annotations\":{\"storageclass.kubernetes.io/is-default-class\":\"false\"}}}' kubectl patch storageclass ebs-sc -p '{\"metadata\":{\"annotations\":{\"storageclass.kubernetes.io/is-default-class\":\"true\"}}}' kubectl get storageclass Define the default storage class A StorageClass provides a way for administrators to describe the \"classes\" of storage they offer. Dynamic Volume Provisioning allows storage volumes and persistent volumes to be created on-demand. StorageClass should be pre-created to define which provisioner should be used and what parameters should be passed when dynamic provisioning is invoked. To define the storage class the following manifest file will be applied. Thanks to this storage class every persistent volume will be deployed in AWS S3, using the previously installed storage controller. apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: ebs-sc-retain provisioner: ebs.csi.aws.com reclaimPolicy: Retain volumeBindingMode: WaitForFirstConsumer To become the storage class the default one: kubectl annotate storageclass gp2 storageclass.kubernetes.io/is-default-class=true Install Cluster Autoscaler The Kubernetes Cluster Autoscaler automatically adjusts the number of nodes in your cluster when pods fail or are rescheduled onto other nodes. The Cluster Autoscaler is typically installed as a Deployment in your cluster. It uses leader election to ensure high availability, but scaling is done by only one replica at a time. Create an IAM policy that grants the permissions that the Cluster Autoscaler requires to use an IAM role. Save the following contents to a file that's named cluster-autoscaler-policy.json . { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"autoscaling:DescribeAutoScalingGroups\", \"autoscaling:DescribeAutoScalingInstances\", \"autoscaling:DescribeLaunchConfigurations\", \"autoscaling:DescribeTags\", \"ec2:DescribeInstanceTypes\", \"ec2:DescribeLaunchTemplateVersions\" ], \"Resource\": [\"*\"] }, { \"Effect\": \"Allow\", \"Action\": [ \"autoscaling:SetDesiredCapacity\", \"autoscaling:TerminateInstanceInAutoScalingGroup\" ], \"Resource\": [\"*\"] } ] } Create the policy with the following command. aws iam create-policy --policy-name AmazonEKSClusterAutoscalerPolicy --policy-document file://cluster-autoscaler-policy.json eksctl create iamserviceaccount --cluster=5gmeta-cloud --namespace=kube-system --name=cluster-autoscaler --attach-policy-arn=arn:aws:iam::<idaws>:policy/AmazonEKSClusterAutoscalerPolicy --override-existing-serviceaccounts --approve #<idaws> refers to the account ID Complete the following steps to deploy the Cluster Autoscaler. We recommend that you review Deployment considerations and optimize the Cluster Autoscaler deployment before you deploy it to a production cluster. curl -o cluster-autoscaler-autodiscover.yaml https://raw.githubusercontent.com/kubernetes/autoscaler/master/cluster-autoscaler/cloudprovider/aws/examples/cluster-autoscaler-autodiscover.yaml Edit the cluster-autoscaler manifest to replace (including <>) with the name of your cluster, and add the following options. --balance-similar-node-groups ensures that there is enough available compute across all availability zones. --skip-nodes-with-system-pods=false ensures that there are no problems with scaling to zero. kubectl apply -f cluster-autoscaler-autodiscover.yaml kubectl annotate serviceaccount cluster-autoscaler -n kube-system eks.amazonaws.com/role-arn=arn:aws:iam::<idaws>:role/AmazonEKSClusterAutoscalerRole --aprove #<idaws> refers to the account ID kubectl patch deployment cluster-autoscaler -n kube-system -p '{\"spec\":{\"template\":{\"metadata\":{\"annotations\":{\"cluster-autoscaler.kubernetes.io/safe-to-evict\": \"false\"}}}}}' export K8S_VERSION=$(kubectl version --short | grep 'Server Version:' | sed 's/[^0-9.]*\\([0-9.]*\\).*/\\1/' | cut -d. -f1,2) export AUTOSCALER_VERSION=$(curl -s \"https://api.github.com/repos/kubernetes/autoscaler/releases\" | grep '\"tag_name\":' | sed -s 's/.*-\\([0-9][0-9\\.]*\\).*/\\1/' | grep -m1 ${K8S_VERSION}) kubectl set image deployment cluster-autoscaler -n kube-system cluster-autoscaler=k8s.gcr.io/autoscaling/cluster-autoscaler:v${AUTOSCALER_VERSION} kubectl -n kube-system logs -f deployment.apps/cluster-autoscaler Modify AWS Security Group for accessing NodePorts (30000-32768) from outside (if needed)","title":"Step 2: Configure Amazon EKS cluster"},{"location":"cloud-modules-deployment/","text":"5GMETA Modules Deployment In the Cloud Platform we have the following modules: Apisix (Gateway) Keycloak (Identity) Dashboard License API Dataflow API Cloud Instance API Discovery API Every module has its own docker image, and a helm chart for deploying it in a Kubernetes cluster. Once we have the EKS cluster properly configured and the rest of the base components deployed (Kafka, DDBBs, etc), we can deploy 5GMETA modules. For doing that: helm repo add 5gmeta-helm https://<id>@raw.githubusercontent.com/5gmetadmin/helmcharts/main/repository helm repo update helm install <name> 5gmeta/<chart_name> -n <organization_namespace> Of course the behaviour of the K8s application can be changed through the values file. Every time we update the docker image, we have to update the helm deployment, for doing that we can run helm upgrade commmand, but I suggest uninstalling and installing the chart to be sure all the components are updated. helm uninstall <name> -n <organization_namespace> helm install <name> 5gmeta/<chart_name> -n <organization_namespace>","title":"5GMETA Modules Deployment"},{"location":"cloud-modules-deployment/#5gmeta-modules-deployment","text":"In the Cloud Platform we have the following modules: Apisix (Gateway) Keycloak (Identity) Dashboard License API Dataflow API Cloud Instance API Discovery API Every module has its own docker image, and a helm chart for deploying it in a Kubernetes cluster. Once we have the EKS cluster properly configured and the rest of the base components deployed (Kafka, DDBBs, etc), we can deploy 5GMETA modules. For doing that: helm repo add 5gmeta-helm https://<id>@raw.githubusercontent.com/5gmetadmin/helmcharts/main/repository helm repo update helm install <name> 5gmeta/<chart_name> -n <organization_namespace> Of course the behaviour of the K8s application can be changed through the values file. Every time we update the docker image, we have to update the helm deployment, for doing that we can run helm upgrade commmand, but I suggest uninstalling and installing the chart to be sure all the components are updated. helm uninstall <name> -n <organization_namespace> helm install <name> 5gmeta/<chart_name> -n <organization_namespace>","title":"5GMETA Modules Deployment"},{"location":"cloud-monitoring/","text":"Cluster monitoring Installing the Kubernetes Metrics Server kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml kubectl get deployment metrics-server -n kube-system kubectl get --raw /metrics Deploying Kubernetes Dashboard kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.4.0/aio/deploy/recommended.yaml cat <<EOF | kubectl apply -f - apiVersion: v1 kind: ServiceAccount metadata: name: admin-user namespace: kubernetes-dashboard EOF cat <<EOF | kubectl apply -f - apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: admin-user roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: admin-user namespace: kubernetes-dashboard EOF kubectl apply -f eks-admin-service-account.yaml To connect to the Kubernetes dashboard, get the token and expose the service: kubectl -n kubernetes-dashboard get secret $(kubectl -n kubernetes-dashboard get sa/admin-user -o jsonpath=\"{.secrets[0].name}\") -o go-template=\"{{.data.token | base64decode}}\" kubectl proxy To access the dashboard endpoint, open the following link with a web browser: http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/#!/login and paste the token got in the first command. The service type of the dashboard can be switched to NodePort so that the dashboard is exposed permanently. Deploying Kube-Prometheus stack The following helm installs the kube-prometheus stack , a collection of Kubernetes manifests, Grafana dashboards, and Prometheus rules combined with documentation and scripts to provide easy to operate end-to-end Kubernetes cluster monitoring with Prometheus using the Prometheus Operator . helm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo update helm install -n monitoring --create-namespace -f values.yaml prometheus-stack prometheus-community/kube-prometheus-stack --version 46.0 The values file used in the helm can be found in the following 5GMETA repository: values.yaml prometheus: service: type: NodePort prometheusSpec: storageSpec: volumeClaimTemplate: spec: accessModes: [\"ReadWriteOnce\"] resources: requests: storage: 20Gi retention: 30d enabled: true prometheusSpec: additionalScrapeConfigs: | - job_name: kafka static_configs: - targets: - kafkacluster-cp-kafka-connect.kafka.svc.cluster.local:5556 - kafkacluster-cp-kafka-rest.kafka.svc.cluster.local:5556 - kafkacluster-cp-ksql-server.kafka.svc.cluster.local:5556 - kafkacluster-cp-schema-registry.kafka.svc.cluster.local:5556 - kafkacluster-cp-zookeeper.kafka.svc.cluster.local:5556 - kafkacluster-cp-kafka.kafka.svc.cluster.local:5556 alertmanager: service: type: NodePort alertmanagerSpec: storage: volumeClaimTemplate: spec: accessModes: [\"ReadWriteOnce\"] resources: requests: storage: 5Gi grafana: service: type: NodePort persistence: enabled: true To expose temporally the ports of Prometheus and Grafana services: PROMETHEUS=$(kubectl get pod -n monitoring -l app.kubernetes.io/name=prometheus -o jsonpath='{.items[0].metadata.name}') GRAFANA=$(kubectl get pod -n monitoring -l app.kubernetes.io/name=grafana -o jsonpath='{.items[0].metadata.name}') kubectl port-forward -n monitoring $PROMETHEUS 9090 & kubectl port-forward -n monitoring $GRAFANA 3000 & If you want to expose them constantly, change the service type to NodePort.","title":"Cluster monitoring"},{"location":"cloud-monitoring/#cluster-monitoring","text":"","title":"Cluster monitoring"},{"location":"cloud-monitoring/#installing-the-kubernetes-metrics-server","text":"kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml kubectl get deployment metrics-server -n kube-system kubectl get --raw /metrics","title":"Installing the Kubernetes Metrics Server"},{"location":"cloud-monitoring/#deploying-kubernetes-dashboard","text":"kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.4.0/aio/deploy/recommended.yaml cat <<EOF | kubectl apply -f - apiVersion: v1 kind: ServiceAccount metadata: name: admin-user namespace: kubernetes-dashboard EOF cat <<EOF | kubectl apply -f - apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: admin-user roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: admin-user namespace: kubernetes-dashboard EOF kubectl apply -f eks-admin-service-account.yaml To connect to the Kubernetes dashboard, get the token and expose the service: kubectl -n kubernetes-dashboard get secret $(kubectl -n kubernetes-dashboard get sa/admin-user -o jsonpath=\"{.secrets[0].name}\") -o go-template=\"{{.data.token | base64decode}}\" kubectl proxy To access the dashboard endpoint, open the following link with a web browser: http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/#!/login and paste the token got in the first command. The service type of the dashboard can be switched to NodePort so that the dashboard is exposed permanently.","title":"Deploying Kubernetes Dashboard"},{"location":"cloud-monitoring/#deploying-kube-prometheus-stack","text":"The following helm installs the kube-prometheus stack , a collection of Kubernetes manifests, Grafana dashboards, and Prometheus rules combined with documentation and scripts to provide easy to operate end-to-end Kubernetes cluster monitoring with Prometheus using the Prometheus Operator . helm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo update helm install -n monitoring --create-namespace -f values.yaml prometheus-stack prometheus-community/kube-prometheus-stack --version 46.0 The values file used in the helm can be found in the following 5GMETA repository: values.yaml prometheus: service: type: NodePort prometheusSpec: storageSpec: volumeClaimTemplate: spec: accessModes: [\"ReadWriteOnce\"] resources: requests: storage: 20Gi retention: 30d enabled: true prometheusSpec: additionalScrapeConfigs: | - job_name: kafka static_configs: - targets: - kafkacluster-cp-kafka-connect.kafka.svc.cluster.local:5556 - kafkacluster-cp-kafka-rest.kafka.svc.cluster.local:5556 - kafkacluster-cp-ksql-server.kafka.svc.cluster.local:5556 - kafkacluster-cp-schema-registry.kafka.svc.cluster.local:5556 - kafkacluster-cp-zookeeper.kafka.svc.cluster.local:5556 - kafkacluster-cp-kafka.kafka.svc.cluster.local:5556 alertmanager: service: type: NodePort alertmanagerSpec: storage: volumeClaimTemplate: spec: accessModes: [\"ReadWriteOnce\"] resources: requests: storage: 5Gi grafana: service: type: NodePort persistence: enabled: true To expose temporally the ports of Prometheus and Grafana services: PROMETHEUS=$(kubectl get pod -n monitoring -l app.kubernetes.io/name=prometheus -o jsonpath='{.items[0].metadata.name}') GRAFANA=$(kubectl get pod -n monitoring -l app.kubernetes.io/name=grafana -o jsonpath='{.items[0].metadata.name}') kubectl port-forward -n monitoring $PROMETHEUS 9090 & kubectl port-forward -n monitoring $GRAFANA 3000 & If you want to expose them constantly, change the service type to NodePort.","title":"Deploying Kube-Prometheus stack"},{"location":"docker-signing/","text":"Signing Docker Images All the image developers should sign every docker image or tag with the following guidelines. Load the docker delegation private key ( 5gmeta.key ) stored in this repository. $ docker trust key load 5gmeta.key --name 5gmeta Passphrase of the key: <password> The loaded private key will be placed under ~/.docker/trust/private Finally, sign the image with: $ docker trust sign 5gmeta/<image>:<tag> This command will sign the image and push it to Docker Hub. The public key of 5gmeta delegations ( 5gmeta.pub ) is already added in the underlying Notary server of Docker Hub, allowing to sign images with the private key (loaded in the previous step) for the repos in the following list: 5gmeta/edgeinstance-api 5gmeta/cloudinstance-api 5gmeta/image-anonymizator 5gmeta/video-anonymizator 5gmeta/dashboard 5gmeta/dataflow-api 5gmeta/video-stream-broker 5gmeta/registration-api 5gmeta/message-converter 5gmeta/discovery-api 5gmeta/slaedge-api 5gmeta/slacloud-api 5gmeta/license-api 5gmeta/kafka-connect 5gmeta/gateway 5gmeta/message-broker 5gmeta/identity-api 5gmeta/helloworld *If you want to sign any image that is not in the previous list, please contact the administrators. Inspecting signed images: We can fetch info from Docker Hub (Notary in Docker registry) about the signatures and signatories status of the repo by: $ docker trust inspect --pretty 5gmeta/<image> This command shows which tags on this repository are signed as well as the list of people with signatures attached to this repository. Deploying images: A signed and an unsigned image have been uploaded to 5GMETA's DockerHub to check how the admission works. The following container can be run to check the behaviour: kubectl run signed-pipeline --image=docker.io/5gmeta/pipeline:signed kubectl run unsigned-pipeline --image=docker.io/5gmeta/pipeline:unsigned Management of Docker Content Trust (FOR ADMINISTRATORS ONLY) \u26a0 \u26d4 Please do not continue reading if you are not involved in the administration of DockerHub. \u26d4 \u26a0 As atated in the previous section the 5gmeta delegation public key has been already added to the Notary server of Docker Hub for every repository, using the root keys stored in the repository. The command used for doing this: $ docker trust signer add --key 5gmeta.pub 5gmeta 5gmeta/<image> The first time a delegation is added to a repository, the command will also initiate the repository using a local Notary canonical root key that has been already created ( root.key ). This includes creating the notary target (or repository) and snapshots keys, and rotating the snapshot key to be managed by the notary server. More information on these keys can be found here . To load and configure the passphrase of the root key: $ docker trust key load root.key $ export DOCKER_CONTENT_TRUST_REPOSITORY_PASSPHRASE=\"<password>\" To add more delegations (public/private keys to an already initialized repository the target or repository keys are needed. They have been compressed in the tar.gz file of the repository. For adding them: $ tar -xvf private_keys_backup.tar.gz -C / The passphrase of these repository keys is the same as always. You can configure an env variable for usign it automatically: $ export DOCKER_CONTENT_TRUST_ROOT_PASSPHRASE=\"<password>\" The previous step is only needed as said for adding more delegations. If this is needed, after loading the target keys run the following commands: $ docker trust key generate <delegation> $ docker trust signer add --key <delegation.pub> <delegation> 5gmeta/<image> The Docker client stores the keys in the ~/.docker/trust/private directory. For backing them up: $ umask 077; tar -zcvf private_keys_backup.tar.gz ~/.docker/trust/private; umask 022 For more information about Docker Content Trust (DCT): https://docs.docker.com/engine/security/trust/ Connaisseur What is Connaisseur? A Kubernetes admission controller to integrate container image signature verification and trust pinning into a cluster. Connaisseur ensures integrity and provenance of container images in a Kubernetes cluster. To do so, it intercepts resource creation or update requests sent to the Kubernetes cluster, identifies all container images and verifies their signatures against pre-configured public keys. Based on the result, it either accepts or denies those requests. To learn more about Connaisseur, visit the full documentation . Helm values Connaisseur is deployed using helm and configured via helm values , so we will start there. We need to set Connaisseur to use the root public key, ( root.pub ) that has been created in the previous section, for validation of the images. To do so, in the .validators section the 5gmeta validator will be created, adding the public root key and DockerHub's credentials. For getting the public root key of any other repository you can check the following tutorial . After that, a new image policy .policy is created to apply the 5gmeta validator to the following pattern: docker.io/5gmeta/*:* . Finally namespaced validation is enabled, to allow ignoring validation in specific namespaces. We leave the rest untouched. # configure Connaisseur deployment deployment: replicasCount: 1 ### VALIDATORS ### # validators are a set of configurations (types, public keys, authentication) # that can be used for validating one or multiple images (or image signatures). # they are tied to their respective image(s) via the image policy below. there # are a few handy validators pre-configured. validators: # static validator that allows each image - name: allow type: static approve: true # static validator that denies each image - name: deny type: static approve: false # 5gmeta validator - name: 5gmeta type: notaryv1 host: notary.docker.io trust_roots: - name: default key: | -----BEGIN PUBLIC KEY----- <XXXXXXXXXXXXXXXXXXXXXXXXXXX> -----END PUBLIC KEY----- auth: username: '<user>' password: '<password>' # the `default` validator is used if no validator is specified in image policy - name: default type: notaryv1 # or other supported validator (e.g. \"cosign\") host: notary.docker.io # configure the notary server for notaryv1 or rekor url for cosign trust_roots: # # the `default` key is used if no key is specified in image policy #- name: default # key: | # enter your public key below # -----BEGIN PUBLIC KEY----- # <add your public key here> # -----END PUBLIC KEY----- #cert: | # in case the trust data host is using a self-signed certificate # -----BEGIN CERTIFICATE----- # ... # -----END CERTIFICATE----- #auth: # credentials in case the trust data requires authentication # # either (preferred solution) # secret_name: mysecret # reference a k8s secret in the form required by the validator type (check the docs) # # or (only for notaryv1 validator) # username: myuser # password: mypass # pre-configured nv1 validator for public notary from Docker Hub - name: dockerhub-basics type: notaryv1 host: notary.docker.io trust_roots: # public key for official docker images (https://hub.docker.com/search?q=&type=image&image_filter=official) # !if not needed feel free to remove the key! - name: docker-official key: | -----BEGIN PUBLIC KEY----- <XXXXXXXXXXXXXXXXXXXXXXXXX> -----END PUBLIC KEY----- # public key securesystemsengineering repo including Connaisseur images # !this key is critical for Connaisseur! - name: securesystemsengineering-official key: | -----BEGIN PUBLIC KEY----- <XXXXXXXXXXXXXXXXXXXXXXXXXXXXX> -----END PUBLIC KEY----- ### IMAGE POLICY ### # the image policy ties validators and images together whereby always only the most specific rule (pattern) # is applied. specify if and how images should be validated by which validator via the validator name. policy: - pattern: \"*:*\" - pattern: \"docker.io/library/*:*\" validator: dockerhub-basics with: trust_root: docker-official - pattern: \"k8s.gcr.io/*:*\" validator: allow - pattern: \"docker.io/securesystemsengineering/*:*\" validator: dockerhub-basics with: trust_root: securesystemsengineering-official - pattern: \"docker.io/5gmeta/*:*\" validator: 5gmeta # in detection mode, deployment will not be denied, but only prompted # and logged. this allows testing the functionality without # interrupting operation. detectionMode: false # namespaced validation allows to restrict the namespaces that will be subject to Connaisseur verification. # when enabled, based on namespaced validation mode ('ignore' or 'validate') # - either all namespaces with label \"securesystemsengineering.connaisseur/webhook=ignore\" are ignored # - or only namespaces with label \"securesystemsengineering.connaisseur/webhook=validate\" are validated. # warning: enabling namespaced validation, allows roles with edit permission on a namespace to disable # validation for that namespace namespacedValidation: enabled: true mode: ignore # 'ignore' or 'validate' Connaisseur is automatically deployed and configured in a MEC server using the Ansible Playbook of this repository, but it could be deployed manually through: $ helm repo add connaisseur https://sse-secure-systems.github.io/connaisseur/charts $ helm install connaisseur connaisseur/connaisseur --atomic --create-namespace --namespace connaisseur -f connaisseur-values.yaml Namespace Validation Namespaced validation allows restricting validation to specific namespaces. Connaisseur will only verify trust of images deployed to the configured namespaces. This can greatly support initial rollout by stepwise extending the validated namespaces or excluding specific namespaces for which signatures are unfeasible. In this case, all namespaces with label securesystemsengineering.connaisseur/webhook: ignore will be ignored. To add or delete the labels, run the following commands: kubectl label namespaces <namespace> securesystemsengineering.connaisseur/webhook=ignore kubectl label namespaces default securesystemsengineering.connaisseur/webhook- Finally to check which namespaces have the ignore label run kubectl get ns --show-labels Test Connaisseur kubectl run signed-pipeline --image=docker.io/5gmeta/pipeline:signed kubectl run unsigned-pipeline --image=docker.io/5gmeta/pipeline:unsigned","title":"Signing Docker Images"},{"location":"docker-signing/#signing-docker-images","text":"All the image developers should sign every docker image or tag with the following guidelines. Load the docker delegation private key ( 5gmeta.key ) stored in this repository. $ docker trust key load 5gmeta.key --name 5gmeta Passphrase of the key: <password> The loaded private key will be placed under ~/.docker/trust/private Finally, sign the image with: $ docker trust sign 5gmeta/<image>:<tag> This command will sign the image and push it to Docker Hub. The public key of 5gmeta delegations ( 5gmeta.pub ) is already added in the underlying Notary server of Docker Hub, allowing to sign images with the private key (loaded in the previous step) for the repos in the following list: 5gmeta/edgeinstance-api 5gmeta/cloudinstance-api 5gmeta/image-anonymizator 5gmeta/video-anonymizator 5gmeta/dashboard 5gmeta/dataflow-api 5gmeta/video-stream-broker 5gmeta/registration-api 5gmeta/message-converter 5gmeta/discovery-api 5gmeta/slaedge-api 5gmeta/slacloud-api 5gmeta/license-api 5gmeta/kafka-connect 5gmeta/gateway 5gmeta/message-broker 5gmeta/identity-api 5gmeta/helloworld *If you want to sign any image that is not in the previous list, please contact the administrators.","title":"Signing Docker Images"},{"location":"docker-signing/#inspecting-signed-images","text":"We can fetch info from Docker Hub (Notary in Docker registry) about the signatures and signatories status of the repo by: $ docker trust inspect --pretty 5gmeta/<image> This command shows which tags on this repository are signed as well as the list of people with signatures attached to this repository.","title":"Inspecting signed images:"},{"location":"docker-signing/#deploying-images","text":"A signed and an unsigned image have been uploaded to 5GMETA's DockerHub to check how the admission works. The following container can be run to check the behaviour: kubectl run signed-pipeline --image=docker.io/5gmeta/pipeline:signed kubectl run unsigned-pipeline --image=docker.io/5gmeta/pipeline:unsigned","title":"Deploying images:"},{"location":"docker-signing/#management-of-docker-content-trust-for-administrators-only","text":"\u26a0 \u26d4 Please do not continue reading if you are not involved in the administration of DockerHub. \u26d4 \u26a0 As atated in the previous section the 5gmeta delegation public key has been already added to the Notary server of Docker Hub for every repository, using the root keys stored in the repository. The command used for doing this: $ docker trust signer add --key 5gmeta.pub 5gmeta 5gmeta/<image> The first time a delegation is added to a repository, the command will also initiate the repository using a local Notary canonical root key that has been already created ( root.key ). This includes creating the notary target (or repository) and snapshots keys, and rotating the snapshot key to be managed by the notary server. More information on these keys can be found here . To load and configure the passphrase of the root key: $ docker trust key load root.key $ export DOCKER_CONTENT_TRUST_REPOSITORY_PASSPHRASE=\"<password>\" To add more delegations (public/private keys to an already initialized repository the target or repository keys are needed. They have been compressed in the tar.gz file of the repository. For adding them: $ tar -xvf private_keys_backup.tar.gz -C / The passphrase of these repository keys is the same as always. You can configure an env variable for usign it automatically: $ export DOCKER_CONTENT_TRUST_ROOT_PASSPHRASE=\"<password>\" The previous step is only needed as said for adding more delegations. If this is needed, after loading the target keys run the following commands: $ docker trust key generate <delegation> $ docker trust signer add --key <delegation.pub> <delegation> 5gmeta/<image> The Docker client stores the keys in the ~/.docker/trust/private directory. For backing them up: $ umask 077; tar -zcvf private_keys_backup.tar.gz ~/.docker/trust/private; umask 022 For more information about Docker Content Trust (DCT): https://docs.docker.com/engine/security/trust/","title":"Management of Docker Content Trust (FOR ADMINISTRATORS ONLY)"},{"location":"docker-signing/#connaisseur","text":"What is Connaisseur? A Kubernetes admission controller to integrate container image signature verification and trust pinning into a cluster. Connaisseur ensures integrity and provenance of container images in a Kubernetes cluster. To do so, it intercepts resource creation or update requests sent to the Kubernetes cluster, identifies all container images and verifies their signatures against pre-configured public keys. Based on the result, it either accepts or denies those requests. To learn more about Connaisseur, visit the full documentation .","title":"Connaisseur"},{"location":"docker-signing/#helm-values","text":"Connaisseur is deployed using helm and configured via helm values , so we will start there. We need to set Connaisseur to use the root public key, ( root.pub ) that has been created in the previous section, for validation of the images. To do so, in the .validators section the 5gmeta validator will be created, adding the public root key and DockerHub's credentials. For getting the public root key of any other repository you can check the following tutorial . After that, a new image policy .policy is created to apply the 5gmeta validator to the following pattern: docker.io/5gmeta/*:* . Finally namespaced validation is enabled, to allow ignoring validation in specific namespaces. We leave the rest untouched. # configure Connaisseur deployment deployment: replicasCount: 1 ### VALIDATORS ### # validators are a set of configurations (types, public keys, authentication) # that can be used for validating one or multiple images (or image signatures). # they are tied to their respective image(s) via the image policy below. there # are a few handy validators pre-configured. validators: # static validator that allows each image - name: allow type: static approve: true # static validator that denies each image - name: deny type: static approve: false # 5gmeta validator - name: 5gmeta type: notaryv1 host: notary.docker.io trust_roots: - name: default key: | -----BEGIN PUBLIC KEY----- <XXXXXXXXXXXXXXXXXXXXXXXXXXX> -----END PUBLIC KEY----- auth: username: '<user>' password: '<password>' # the `default` validator is used if no validator is specified in image policy - name: default type: notaryv1 # or other supported validator (e.g. \"cosign\") host: notary.docker.io # configure the notary server for notaryv1 or rekor url for cosign trust_roots: # # the `default` key is used if no key is specified in image policy #- name: default # key: | # enter your public key below # -----BEGIN PUBLIC KEY----- # <add your public key here> # -----END PUBLIC KEY----- #cert: | # in case the trust data host is using a self-signed certificate # -----BEGIN CERTIFICATE----- # ... # -----END CERTIFICATE----- #auth: # credentials in case the trust data requires authentication # # either (preferred solution) # secret_name: mysecret # reference a k8s secret in the form required by the validator type (check the docs) # # or (only for notaryv1 validator) # username: myuser # password: mypass # pre-configured nv1 validator for public notary from Docker Hub - name: dockerhub-basics type: notaryv1 host: notary.docker.io trust_roots: # public key for official docker images (https://hub.docker.com/search?q=&type=image&image_filter=official) # !if not needed feel free to remove the key! - name: docker-official key: | -----BEGIN PUBLIC KEY----- <XXXXXXXXXXXXXXXXXXXXXXXXX> -----END PUBLIC KEY----- # public key securesystemsengineering repo including Connaisseur images # !this key is critical for Connaisseur! - name: securesystemsengineering-official key: | -----BEGIN PUBLIC KEY----- <XXXXXXXXXXXXXXXXXXXXXXXXXXXXX> -----END PUBLIC KEY----- ### IMAGE POLICY ### # the image policy ties validators and images together whereby always only the most specific rule (pattern) # is applied. specify if and how images should be validated by which validator via the validator name. policy: - pattern: \"*:*\" - pattern: \"docker.io/library/*:*\" validator: dockerhub-basics with: trust_root: docker-official - pattern: \"k8s.gcr.io/*:*\" validator: allow - pattern: \"docker.io/securesystemsengineering/*:*\" validator: dockerhub-basics with: trust_root: securesystemsengineering-official - pattern: \"docker.io/5gmeta/*:*\" validator: 5gmeta # in detection mode, deployment will not be denied, but only prompted # and logged. this allows testing the functionality without # interrupting operation. detectionMode: false # namespaced validation allows to restrict the namespaces that will be subject to Connaisseur verification. # when enabled, based on namespaced validation mode ('ignore' or 'validate') # - either all namespaces with label \"securesystemsengineering.connaisseur/webhook=ignore\" are ignored # - or only namespaces with label \"securesystemsengineering.connaisseur/webhook=validate\" are validated. # warning: enabling namespaced validation, allows roles with edit permission on a namespace to disable # validation for that namespace namespacedValidation: enabled: true mode: ignore # 'ignore' or 'validate' Connaisseur is automatically deployed and configured in a MEC server using the Ansible Playbook of this repository, but it could be deployed manually through: $ helm repo add connaisseur https://sse-secure-systems.github.io/connaisseur/charts $ helm install connaisseur connaisseur/connaisseur --atomic --create-namespace --namespace connaisseur -f connaisseur-values.yaml","title":"Helm values"},{"location":"docker-signing/#namespace-validation","text":"Namespaced validation allows restricting validation to specific namespaces. Connaisseur will only verify trust of images deployed to the configured namespaces. This can greatly support initial rollout by stepwise extending the validated namespaces or excluding specific namespaces for which signatures are unfeasible. In this case, all namespaces with label securesystemsengineering.connaisseur/webhook: ignore will be ignored. To add or delete the labels, run the following commands: kubectl label namespaces <namespace> securesystemsengineering.connaisseur/webhook=ignore kubectl label namespaces default securesystemsengineering.connaisseur/webhook- Finally to check which namespaces have the ignore label run kubectl get ns --show-labels","title":"Namespace Validation"},{"location":"docker-signing/#test-connaisseur","text":"kubectl run signed-pipeline --image=docker.io/5gmeta/pipeline:signed kubectl run unsigned-pipeline --image=docker.io/5gmeta/pipeline:unsigned","title":"Test Connaisseur"},{"location":"gpu-management/","text":"Kubernetes GPU Management GPU Operator The NVIDIA GPU Operator automatically setups and manages the NVIDIA software components on the worker nodes. For installing the operator the official documentacion has been followed: https://docs.nvidia.com/datacenter/cloud-native/kubernetes/install-k8s.html#step-4-setup-nvidia-software The GPU worker nodes in the Kubernetes cluster need to be enabled with the following components: NVIDIA drivers NVIDIA Container Toolkit NVIDIA Kubernetes Device Plugin (and optionally GPU Feature Discovery plugin) (Optional) DCGM-Exporter to gather GPU telemetry and integrate into a monitoring stack such as Prometheus NVIDIA drivers sudo apt-get install linux-headers-$(uname -r) distribution=$(. /etc/os-release;echo $ID$VERSION_ID | sed -e 's/\\.//g') \\ && wget https://developer.download.nvidia.com/compute/cuda/repos/$distribution/x86_64/cuda-$distribution.pin \\ && sudo mv cuda-$distribution.pin /etc/apt/preferences.d/cuda-repository-pin-600 sudo apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/$distribution/x86_64/7fa2af80.pub \\ && echo \"deb http://developer.download.nvidia.com/compute/cuda/repos/$distribution/x86_64 /\" | sudo tee /etc/apt/sources.list.d/cuda.list sudo apt-get update && sudo apt-get -y install cuda-drivers NVIDIA Container Toolkit distribution=$(. /etc/os-release;echo $ID$VERSION_ID) \\ && curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add - \\ && curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list sudo apt-get update \\ && sudo apt-get install -y nvidia-docker2 sudo nano /etc/docker/daemon.json # add the following: { \"default-runtime\": \"nvidia\", \"runtimes\": { \"nvidia\": { \"path\": \"/usr/bin/nvidia-container-runtime\", \"runtimeArgs\": [] } } } sudo systemctl restart docker NVIDIA Kubernetes Device Plugin curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 \\ && chmod 700 get_helm.sh \\ && ./get_helm.sh helm repo add nvdp https://nvidia.github.io/k8s-device-plugin \\ && helm repo update helm install --generate-name nvdp/nvidia-device-plugin GPU Monitoring DCGM-Exporter DCGM-Exporter is a tool based on the Go APIs to NVIDIA DCGM that allows users to gather GPU metrics and understand workload behavior or monitor GPUs in clusters. dcgm-exporter is written in Go and exposes GPU metrics at an HTTP endpoint ( /metrics ) for monitoring solutions such as Prometheus. For information on the profiling metrics available from DCGM, refer to this section in the official documentation. helm repo add gpu-helm-charts https://nvidia.github.io/dcgm-exporter/helm-charts helm repo update helm install --generate-name gpu-helm-charts/dcgm-exporter GPU Metrics in Grafana Dashboard to be imported in Grafana, https://grafana.com/grafana/dashboards/12239","title":"Kubernetes GPU Management"},{"location":"gpu-management/#kubernetes-gpu-management","text":"","title":"Kubernetes GPU Management"},{"location":"gpu-management/#gpu-operator","text":"The NVIDIA GPU Operator automatically setups and manages the NVIDIA software components on the worker nodes. For installing the operator the official documentacion has been followed: https://docs.nvidia.com/datacenter/cloud-native/kubernetes/install-k8s.html#step-4-setup-nvidia-software The GPU worker nodes in the Kubernetes cluster need to be enabled with the following components: NVIDIA drivers NVIDIA Container Toolkit NVIDIA Kubernetes Device Plugin (and optionally GPU Feature Discovery plugin) (Optional) DCGM-Exporter to gather GPU telemetry and integrate into a monitoring stack such as Prometheus NVIDIA drivers sudo apt-get install linux-headers-$(uname -r) distribution=$(. /etc/os-release;echo $ID$VERSION_ID | sed -e 's/\\.//g') \\ && wget https://developer.download.nvidia.com/compute/cuda/repos/$distribution/x86_64/cuda-$distribution.pin \\ && sudo mv cuda-$distribution.pin /etc/apt/preferences.d/cuda-repository-pin-600 sudo apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/$distribution/x86_64/7fa2af80.pub \\ && echo \"deb http://developer.download.nvidia.com/compute/cuda/repos/$distribution/x86_64 /\" | sudo tee /etc/apt/sources.list.d/cuda.list sudo apt-get update && sudo apt-get -y install cuda-drivers NVIDIA Container Toolkit distribution=$(. /etc/os-release;echo $ID$VERSION_ID) \\ && curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add - \\ && curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list sudo apt-get update \\ && sudo apt-get install -y nvidia-docker2 sudo nano /etc/docker/daemon.json # add the following: { \"default-runtime\": \"nvidia\", \"runtimes\": { \"nvidia\": { \"path\": \"/usr/bin/nvidia-container-runtime\", \"runtimeArgs\": [] } } } sudo systemctl restart docker NVIDIA Kubernetes Device Plugin curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 \\ && chmod 700 get_helm.sh \\ && ./get_helm.sh helm repo add nvdp https://nvidia.github.io/k8s-device-plugin \\ && helm repo update helm install --generate-name nvdp/nvidia-device-plugin","title":"GPU Operator"},{"location":"gpu-management/#gpu-monitoring","text":"DCGM-Exporter DCGM-Exporter is a tool based on the Go APIs to NVIDIA DCGM that allows users to gather GPU metrics and understand workload behavior or monitor GPUs in clusters. dcgm-exporter is written in Go and exposes GPU metrics at an HTTP endpoint ( /metrics ) for monitoring solutions such as Prometheus. For information on the profiling metrics available from DCGM, refer to this section in the official documentation. helm repo add gpu-helm-charts https://nvidia.github.io/dcgm-exporter/helm-charts helm repo update helm install --generate-name gpu-helm-charts/dcgm-exporter GPU Metrics in Grafana Dashboard to be imported in Grafana, https://grafana.com/grafana/dashboards/12239","title":"GPU Monitoring"},{"location":"kafka-deployment/","text":"Kafka Infraestructure Infraestructure deployment For deploying Kafka Platform, the solution from Confluent has been used: https://github.com/confluentinc/cp-helm-charts Several changes have been made in the helm chart provided by Confluent. For example, Kafka UI has been added to the deployment files, so that is is deployed together with the rest of components. A service monitor for monitoring kafka has been also added. The modified helm chart can be found in the 5GMETA's repositories: https://github.com/5gmeta/stream-data-gateway/tree/main/src/prod-version To install Kafka Platform helm chart, first clone the previous repository and go to prod-version folder. Then install it: git clone https://github.com/5gmeta/stream-data-gateway.git cd stream-data-gateway/src/prod-version helm install kafkacluster ./cp-helm-charts -n kafka If for some reason the official (no modified )helm has to be installed: helm repo add confluentinc https://confluentinc.github.io/cp-helm-charts/ helm repo update helm install confluentinc/cp-helm-charts --name kafka --set cp-control-center.enabled=false Confluent Kafka Grafana dashboard: To be imported in Grafana, https://github.com/5gmeta/stream-data-gateway/blob/main/src/prod-version/cp-helm-charts/5GMETA/confluent-open-source-grafana-dashboard.json Kafka-UI manifest for deploying the UI: It is deployed through the helm, the followinf manifest is the one that is being deployed: --- apiVersion: apps/v1 kind: Deployment metadata: name: kafka-ui labels: app.kubernetes.io/component: kafka-ui namespace: kafka spec: replicas: 1 selector: matchLabels: app.kubernetes.io/component: kafka-ui template: metadata: labels: app.kubernetes.io/component: kafka-ui spec: containers: - name: kafka-ui image: provectuslabs/kafka-ui:latest imagePullPolicy: Always ports: - name: http containerPort: 8080 protocol: TCP env: - name: KAFKA_CLUSTERS_0_NAME value: 5gmeta-cloud - name: KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS value: kafkacluster-cp-kafka.kafka.svc.cluster.local:9092 # - name: KAFKA_CLUSTERS_0_ZOOKEEPER # value: kafkacluster-cp-zookeeper.kafka.svc.cluster.local:2181 - name: KAFKA_CLUSTERS_0_SCHEMAREGISTRY value: http://kafkacluster-cp-schema-registry.kafka.svc.cluster.local:8081 - name: KAFKA_CLUSTERS_0_KAFKACONNECT_0_NAME value: kafka-connect - name: KAFKA_CLUSTERS_0_KAFKACONNECT_0_ADDRESS value: http://kafkacluster-cp-kafka-connect.kafka.svc.cluster.local:8083 - name: KAFKA_CLUSTERS_0_KSQLDBSERVER value: http://kafkacluster-cp-ksql-server.kafka.svc.cluster.local:8088 - name: AUTH_TYPE value: \"LOGIN_FORM\" - name: SPRING_SECURITY_USER_NAME value: <user> - name: SPRING_SECURITY_USER_PASSWORD value: <password> --- apiVersion: v1 kind: Service metadata: name: kafka-ui labels: app.kubernetes.io/component: kafka-ui namespace: kafka spec: type: NodePort ports: - port: 8080 protocol: TCP targetPort: http nodePort: 31080 selector: app.kubernetes.io/component: kafka-ui KSQL DB For deploying a KSQL DB Client and run SQL commands, the following command can be used: kubectl run tmp-ksql-cli -n kafka --rm -i --tty --image confluentinc/cp-ksqldb-cli:7.1.0 [http://kafkacluster-cp-ksql-server:8088](http://kafkacluster-cp-ksql-server:8088) The commands can be also run though Kafka UI. Kafka Connect Connectors configuration code base: Upload Connector: { \"connector.class\": \"com.datamountaineer.streamreactor.connect.jms.source.JMSSourceConnector\", \"connect.jms.initial.context.factory\": \"org.apache.activemq.jndi.ActiveMQInitialContextFactory\", \"tasks.max\": 1, \"connect.jms.url\": \"tcp://<IP>:<PORT>\", \"connect.jms.username\": \"<user>\", \"connect.jms.connection.factory\": \"ConnectionFactory\", \"connect.jms.queues\": \"jms-queue\", \"connect.jms.password\": \"<password>\", \"connect.progress.enabled\": \"true\", \"connect.jms.kcql\": \"INSERT INTO \"<datatype>-<instance_type>\" SELECT * FROM \"<datatype>-<instance_type>\" WITHTYPE TOPIC \", \"name\": \"<CONNECTOR NAME>\" } Download Connector: { \"connector.class\": \"com.datamountaineer.streamreactor.connect.jms.sink.JMSSinkConnector\", \"connect.jms.initial.context.factory\": \"org.apache.activemq.jndi.ActiveMQInitialContextFactory\", \"tasks.max\": 1, \"connect.jms.url\": \"tcp://<IP>:<PORT>\", \"connect.jms.username\": \"<user>\", \"connect.jms.connection.factory\": \"ConnectionFactory\", \"connect.jms.queues\": \"jms-queue\", \"connect.jms.password\": \"<password>\", \"connect.progress.enabled\": \"true\", \"connect.jms.kcql\": \"INSERT INTO event SELECT * FROM event WITHTYPE TOPIC WITHFORMAT JSON \", \"topics\": \"event\", \"name\": \"<CONNECTOR NAME>\" } Schema Registry Schema for < datatype>- topics. The name of the schema must be < datatype>- -value . { \"type\": \"record\", \"name\": \"jms\", \"namespace\": \"com.datamountaineer.streamreactor.connect\", \"fields\": [ { \"name\": \"message_timestamp\", \"type\": [ \"null\", \"long\" ], \"default\": null }, { \"name\": \"correlation_id\", \"type\": [ \"null\", \"string\" ], \"default\": null }, { \"name\": \"redelivered\", \"type\": [ \"null\", \"boolean\" ], \"default\": null }, { \"name\": \"reply_to\", \"type\": [ \"null\", \"string\" ], \"default\": null }, { \"name\": \"destination\", \"type\": [ \"null\", \"string\" ], \"default\": null }, { \"name\": \"message_id\", \"type\": [ \"null\", \"string\" ], \"default\": null }, { \"name\": \"mode\", \"type\": [ \"null\", \"int\" ], \"default\": null }, { \"name\": \"type\", \"type\": [ \"null\", \"string\" ], \"default\": null }, { \"name\": \"priority\", \"type\": [ \"null\", \"int\" ], \"default\": null }, { \"name\": \"bytes_payload\", \"type\": [ \"null\", \"bytes\" ], \"default\": null }, { \"name\": \"properties\", \"type\": [ \"null\", { \"type\": \"map\", \"values\": [ \"null\", \"string\" ] } ], \"default\": null } ], \"connect.name\": \"com.datamountaineer.streamreactor.connect.jms\" }","title":"Kafka Infraestructure"},{"location":"kafka-deployment/#kafka-infraestructure","text":"","title":"Kafka Infraestructure"},{"location":"kafka-deployment/#infraestructure-deployment","text":"For deploying Kafka Platform, the solution from Confluent has been used: https://github.com/confluentinc/cp-helm-charts Several changes have been made in the helm chart provided by Confluent. For example, Kafka UI has been added to the deployment files, so that is is deployed together with the rest of components. A service monitor for monitoring kafka has been also added. The modified helm chart can be found in the 5GMETA's repositories: https://github.com/5gmeta/stream-data-gateway/tree/main/src/prod-version To install Kafka Platform helm chart, first clone the previous repository and go to prod-version folder. Then install it: git clone https://github.com/5gmeta/stream-data-gateway.git cd stream-data-gateway/src/prod-version helm install kafkacluster ./cp-helm-charts -n kafka If for some reason the official (no modified )helm has to be installed: helm repo add confluentinc https://confluentinc.github.io/cp-helm-charts/ helm repo update helm install confluentinc/cp-helm-charts --name kafka --set cp-control-center.enabled=false Confluent Kafka Grafana dashboard: To be imported in Grafana, https://github.com/5gmeta/stream-data-gateway/blob/main/src/prod-version/cp-helm-charts/5GMETA/confluent-open-source-grafana-dashboard.json Kafka-UI manifest for deploying the UI: It is deployed through the helm, the followinf manifest is the one that is being deployed: --- apiVersion: apps/v1 kind: Deployment metadata: name: kafka-ui labels: app.kubernetes.io/component: kafka-ui namespace: kafka spec: replicas: 1 selector: matchLabels: app.kubernetes.io/component: kafka-ui template: metadata: labels: app.kubernetes.io/component: kafka-ui spec: containers: - name: kafka-ui image: provectuslabs/kafka-ui:latest imagePullPolicy: Always ports: - name: http containerPort: 8080 protocol: TCP env: - name: KAFKA_CLUSTERS_0_NAME value: 5gmeta-cloud - name: KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS value: kafkacluster-cp-kafka.kafka.svc.cluster.local:9092 # - name: KAFKA_CLUSTERS_0_ZOOKEEPER # value: kafkacluster-cp-zookeeper.kafka.svc.cluster.local:2181 - name: KAFKA_CLUSTERS_0_SCHEMAREGISTRY value: http://kafkacluster-cp-schema-registry.kafka.svc.cluster.local:8081 - name: KAFKA_CLUSTERS_0_KAFKACONNECT_0_NAME value: kafka-connect - name: KAFKA_CLUSTERS_0_KAFKACONNECT_0_ADDRESS value: http://kafkacluster-cp-kafka-connect.kafka.svc.cluster.local:8083 - name: KAFKA_CLUSTERS_0_KSQLDBSERVER value: http://kafkacluster-cp-ksql-server.kafka.svc.cluster.local:8088 - name: AUTH_TYPE value: \"LOGIN_FORM\" - name: SPRING_SECURITY_USER_NAME value: <user> - name: SPRING_SECURITY_USER_PASSWORD value: <password> --- apiVersion: v1 kind: Service metadata: name: kafka-ui labels: app.kubernetes.io/component: kafka-ui namespace: kafka spec: type: NodePort ports: - port: 8080 protocol: TCP targetPort: http nodePort: 31080 selector: app.kubernetes.io/component: kafka-ui","title":"Infraestructure deployment"},{"location":"kafka-deployment/#ksql-db","text":"For deploying a KSQL DB Client and run SQL commands, the following command can be used: kubectl run tmp-ksql-cli -n kafka --rm -i --tty --image confluentinc/cp-ksqldb-cli:7.1.0 [http://kafkacluster-cp-ksql-server:8088](http://kafkacluster-cp-ksql-server:8088) The commands can be also run though Kafka UI.","title":"KSQL DB"},{"location":"kafka-deployment/#kafka-connect","text":"Connectors configuration code base: Upload Connector: { \"connector.class\": \"com.datamountaineer.streamreactor.connect.jms.source.JMSSourceConnector\", \"connect.jms.initial.context.factory\": \"org.apache.activemq.jndi.ActiveMQInitialContextFactory\", \"tasks.max\": 1, \"connect.jms.url\": \"tcp://<IP>:<PORT>\", \"connect.jms.username\": \"<user>\", \"connect.jms.connection.factory\": \"ConnectionFactory\", \"connect.jms.queues\": \"jms-queue\", \"connect.jms.password\": \"<password>\", \"connect.progress.enabled\": \"true\", \"connect.jms.kcql\": \"INSERT INTO \"<datatype>-<instance_type>\" SELECT * FROM \"<datatype>-<instance_type>\" WITHTYPE TOPIC \", \"name\": \"<CONNECTOR NAME>\" } Download Connector: { \"connector.class\": \"com.datamountaineer.streamreactor.connect.jms.sink.JMSSinkConnector\", \"connect.jms.initial.context.factory\": \"org.apache.activemq.jndi.ActiveMQInitialContextFactory\", \"tasks.max\": 1, \"connect.jms.url\": \"tcp://<IP>:<PORT>\", \"connect.jms.username\": \"<user>\", \"connect.jms.connection.factory\": \"ConnectionFactory\", \"connect.jms.queues\": \"jms-queue\", \"connect.jms.password\": \"<password>\", \"connect.progress.enabled\": \"true\", \"connect.jms.kcql\": \"INSERT INTO event SELECT * FROM event WITHTYPE TOPIC WITHFORMAT JSON \", \"topics\": \"event\", \"name\": \"<CONNECTOR NAME>\" }","title":"Kafka Connect"},{"location":"kafka-deployment/#schema-registry","text":"Schema for < datatype>- topics. The name of the schema must be < datatype>- -value . { \"type\": \"record\", \"name\": \"jms\", \"namespace\": \"com.datamountaineer.streamreactor.connect\", \"fields\": [ { \"name\": \"message_timestamp\", \"type\": [ \"null\", \"long\" ], \"default\": null }, { \"name\": \"correlation_id\", \"type\": [ \"null\", \"string\" ], \"default\": null }, { \"name\": \"redelivered\", \"type\": [ \"null\", \"boolean\" ], \"default\": null }, { \"name\": \"reply_to\", \"type\": [ \"null\", \"string\" ], \"default\": null }, { \"name\": \"destination\", \"type\": [ \"null\", \"string\" ], \"default\": null }, { \"name\": \"message_id\", \"type\": [ \"null\", \"string\" ], \"default\": null }, { \"name\": \"mode\", \"type\": [ \"null\", \"int\" ], \"default\": null }, { \"name\": \"type\", \"type\": [ \"null\", \"string\" ], \"default\": null }, { \"name\": \"priority\", \"type\": [ \"null\", \"int\" ], \"default\": null }, { \"name\": \"bytes_payload\", \"type\": [ \"null\", \"bytes\" ], \"default\": null }, { \"name\": \"properties\", \"type\": [ \"null\", { \"type\": \"map\", \"values\": [ \"null\", \"string\" ] } ], \"default\": null } ], \"connect.name\": \"com.datamountaineer.streamreactor.connect.jms\" }","title":"Schema Registry"},{"location":"mec-deployment/","text":"Edge Stack Deployment In order to easily deploy all the components forming the 5GMETA Edge Stack, an unique Ansible playbook will be used. The following playbook will install all the dependencies and components and repositories necessary for the 5GMETA platform's MEC server. Furthermore, every time a change is made in the stack, it will be reflected in the playbook and running it again will let to update the deployment without extra effort. The only requeriment to deploy the stack is an Ubuntu 20.04 image. I can also be an Ubuntu 18.04 image, but in that case OSM 10 will be installed instead OSM 11. The server needs at least 16 GB of RAM, 4 vCPUs and 60 GB of disk. The playbook will the deploy the following components: Docker Engine Single-node Kubernetes cluster (v1.20.11) Composed by Flannel CNI, OpenEBS Storage and MetalLB Load-balancer Helm k8s application manager Kube-prometheus-stack and Kube-eagle for k8s monitoring Prometheus, Grafana and dashboards MySQL cluster Open Source MANO (OSM): v11 for Ubuntu 20.04 and v10 for Ubuntu 18.04 Notary and Connaisseur for managing security in the cluster 5GMETA MEC APIs and Base Components Installing requirements To run the Ansible playbook, some packages & Ansible collections must be installed before in the control node: sudo apt-add-repository ppa:ansible/ansible sudo apt update sudo apt-get install curl python3-pip ansible ansible-galaxy collection install community.general community.docker kubernetes.core In Ubuntu 18.04: sudo apt-get install python3-distutils python3 get-pip.py python3 -m pip install ansible ansible-galaxy collection install community.general community.docker kubernetes.core For RP1, the public IP of the server should be whitelisted, please contact Vicomtech to add it to the list. Deploying the Edge Stack After installing the previous requirements, the Ansible playbook should be downloaded: curl -s https://raw.githubusercontent.com/5gmeta/orchestrator/main/deploy/EdgeDeployment.yaml -o EdgeDeployment.yaml Then, before running it, the playbook's hosts field or Ansible's inventory file must be modified to match the Ubuntu machine. Vars section must be also filled with the user provided values. After that it can be runned: ansible-playbook EdgeDeployment.yaml Once deployed you can access the different services in the next ports: K8s API in port 6443 K8s UI in port 8080 OSM UI in port 80 OSM API (Orchestration API) in port 9999 Grafana UI in port 3000 Prometheus UI in port 9090 Alert Manager UI in port 9093 5GMETA Edge Instance API in port 5000 5GMETA Registration API in port 12346 5GMETA Message-Broker in port 5673 (SB) and 61616 (NB) 5GMETA Video-Broker in port 8443 Also you can check the status of OSM ressources managed by Kubernetes in the following way: kubectl get all -n osm Re-deploying starting from a specific task of the Ansible playbook If the deployement fails for any reason and you want to play again the playbook starting from a specific task, then you do the following: # For running the playbook starting from the \"Get IP geolocation data\" task ansible-playbook EdgeDeployment.yaml --start-at-task=\"Get IP geolocation data\" Executing only a pecific task of the Ansible playbook If instead you want to execute a single task of the playbook, then you can force Ansible to ask for a confirmation before executing the next step, and abort execution when needed: # For running only the \"Get IP geolocation data\" task, abort execution (ctrl+c) when asked to confirm execution of the following step ansible-playbook playbook.yml --step --start-at-task=\"install packages\" Cleaning deployment To clean the deployment, use the cleanDeployment.sh script. Known/Potential issues related to Ansible playbook execution You are runninng the installer as root This error can be resolved by running the playbook with the current user and not the root user , while having sudo privileges: sudo -u $USER ansible-playbook EdgeDeployment.yaml --ask-become-pass Couldn't resolve module/action 'community.general.ipinfoio_facts' This error is due to the usage of an old version of ansible, and can be easily resolved through: sudo apt purge ansible sudo apt-add-repository ppa:ansible/ansible sudo apt update sudo apt install ansible ansible-galaxy collection install community.general community.docker kubernetes.core --force Cannot uninstall 'PyYAML' PyYAML is potentially installed and already bundled into some core packages of the distro. To force installation of required PyYAML version: sudo pip install --ignore-installed PyYAML Known/Potential issues related to Docker Docker-credential-secretservice failure If the playbook fails in Docker login task (e.g Docker-credential-secretservice fails on Ubuntu 18.04: error getting credentials - err: exit status 1, out:GDBus.Error:org.freedesktop.DBus.Error.ServiceUnknown: The name org.freedesktop.secrets was not provided by any .service files :), run the following commannd and then the ansible again: sudo apt install gnupg2 passls -la Known/Potential issues related to Kubernetes Failure of Kubernetes Cluster init due to unvalidated docker version This error is due to the usage of an unvalidated docker version. To fix it add the following arguments like follows: # Not working - kubeadm init --config {{ ansible_env.HOME }}/5gmeta/tmp/cluster-config.yaml > {{ ansible_env.HOME }}/5gmeta/logs/cluster_init # Working - kubeadm init --ignore-preflight-errors=SystemVerification --config {{ ansible_env.HOME }}/5gmeta/tmp/cluster-config.yaml > {{ ansible_env.HOME }}/5gmeta/logs/cluster_init Failure with validating the existence and emptiness of directory /var/lib/etcd This error can be easily fixed by removing the directory: sudo rm /var/lib/etcd Known/Potential issues related to OSM Failure of OSM install with several deployements still pending If OSM installation fails due to pending deployements, like below: 5 of 9 deployments starting: lcm 0/1 0 mon 0/1 0 nbi 0/1 0 pol 0/1 0 ro 0/1 0 SYSTEM IS BROKEN OSM is not healthy, but will probably converge to a healthy state soon. Check OSM status with: kubectl -n osm get all DONE Identifying the root cause can be difficult. Still a potential solution is to check the logs related to such deployements like below: sudo kubectl get all -n osm #command output!!! NAME READY STATUS RESTARTS AGE pod/grafana-855d96c47d-w4xfs 2/2 Running 0 66m pod/kafka-0 1/1 Running 3 66m pod/keystone-7d9864f8f5-qx4dd 1/1 Running 0 66m pod/lcm-556b46f977-c2ljl 0/1 Init:0/1 0 66m pod/modeloperator-946f64449-mffbc 1/1 Running 0 66m pod/mon-577c8fc975-9kndp 0/1 Init:0/1 0 66m pod/mysql-0 1/1 Running 0 66m pod/nbi-7886459c9f-mldhw 0/1 Init:0/1 0 66m pod/ng-ui-7d98c6568f-d8862 1/1 Running 0 66m pod/pol-7f86867d95-vm762 0/1 Init:0/1 0 66m pod/prometheus-0 1/1 Running 0 66m pod/ro-ddbc9f888-6jmlp 0/1 Init:0/1 0 66m pod/zookeeper-0 1/1 Running 0 66m Then it is possible to get details related to a specific deployment: kubectl describe pod/lcm-556b46f977-c2ljl -n osm #command output!!! Name: lcm-556b46f977-c2ljl Namespace: osm Priority: 0 .... .... Init Containers: kafka-ro-mongo-test: Container ID: docker://8c4fb0ce225af5c4419441c21b1493da252441b83fbe963d4ef7125c44389591 Image: alpine:latest Image ID: docker-pullable:// .... .... And finally to check its logs and identify the root cause kubectl logs pod/lcm-556b46f977-c2ljl-n osm -c kafka-ro-mongo-test #command output!!! nc: bad address 'mongodb-k8s' kafka (10.244.0.25:9092) open nc: bad address 'mongodb-k8s' kafka (10.244.0.25:9092) open OSM command returning \"ModuleNotFoundError: No module named 'prettytable' This error can be easily fixed by: sudo apt install python3-prettytable OSM command returning TypeError: init () got an unexpected keyword argument 'case_sensitive' This error can be easily fixed by: pip3 install -U click","title":"Edge Stack Deployment"},{"location":"mec-deployment/#edge-stack-deployment","text":"In order to easily deploy all the components forming the 5GMETA Edge Stack, an unique Ansible playbook will be used. The following playbook will install all the dependencies and components and repositories necessary for the 5GMETA platform's MEC server. Furthermore, every time a change is made in the stack, it will be reflected in the playbook and running it again will let to update the deployment without extra effort. The only requeriment to deploy the stack is an Ubuntu 20.04 image. I can also be an Ubuntu 18.04 image, but in that case OSM 10 will be installed instead OSM 11. The server needs at least 16 GB of RAM, 4 vCPUs and 60 GB of disk. The playbook will the deploy the following components: Docker Engine Single-node Kubernetes cluster (v1.20.11) Composed by Flannel CNI, OpenEBS Storage and MetalLB Load-balancer Helm k8s application manager Kube-prometheus-stack and Kube-eagle for k8s monitoring Prometheus, Grafana and dashboards MySQL cluster Open Source MANO (OSM): v11 for Ubuntu 20.04 and v10 for Ubuntu 18.04 Notary and Connaisseur for managing security in the cluster 5GMETA MEC APIs and Base Components","title":"Edge Stack Deployment"},{"location":"mec-deployment/#installing-requirements","text":"To run the Ansible playbook, some packages & Ansible collections must be installed before in the control node: sudo apt-add-repository ppa:ansible/ansible sudo apt update sudo apt-get install curl python3-pip ansible ansible-galaxy collection install community.general community.docker kubernetes.core In Ubuntu 18.04: sudo apt-get install python3-distutils python3 get-pip.py python3 -m pip install ansible ansible-galaxy collection install community.general community.docker kubernetes.core For RP1, the public IP of the server should be whitelisted, please contact Vicomtech to add it to the list.","title":"Installing requirements"},{"location":"mec-deployment/#deploying-the-edge-stack","text":"After installing the previous requirements, the Ansible playbook should be downloaded: curl -s https://raw.githubusercontent.com/5gmeta/orchestrator/main/deploy/EdgeDeployment.yaml -o EdgeDeployment.yaml Then, before running it, the playbook's hosts field or Ansible's inventory file must be modified to match the Ubuntu machine. Vars section must be also filled with the user provided values. After that it can be runned: ansible-playbook EdgeDeployment.yaml Once deployed you can access the different services in the next ports: K8s API in port 6443 K8s UI in port 8080 OSM UI in port 80 OSM API (Orchestration API) in port 9999 Grafana UI in port 3000 Prometheus UI in port 9090 Alert Manager UI in port 9093 5GMETA Edge Instance API in port 5000 5GMETA Registration API in port 12346 5GMETA Message-Broker in port 5673 (SB) and 61616 (NB) 5GMETA Video-Broker in port 8443 Also you can check the status of OSM ressources managed by Kubernetes in the following way: kubectl get all -n osm","title":"Deploying the Edge Stack"},{"location":"mec-deployment/#re-deploying-starting-from-a-specific-task-of-the-ansible-playbook","text":"If the deployement fails for any reason and you want to play again the playbook starting from a specific task, then you do the following: # For running the playbook starting from the \"Get IP geolocation data\" task ansible-playbook EdgeDeployment.yaml --start-at-task=\"Get IP geolocation data\"","title":"Re-deploying starting from a specific task of the Ansible playbook"},{"location":"mec-deployment/#executing-only-a-pecific-task-of-the-ansible-playbook","text":"If instead you want to execute a single task of the playbook, then you can force Ansible to ask for a confirmation before executing the next step, and abort execution when needed: # For running only the \"Get IP geolocation data\" task, abort execution (ctrl+c) when asked to confirm execution of the following step ansible-playbook playbook.yml --step --start-at-task=\"install packages\"","title":"Executing only a pecific task of the Ansible playbook"},{"location":"mec-deployment/#cleaning-deployment","text":"To clean the deployment, use the cleanDeployment.sh script.","title":"Cleaning deployment"},{"location":"mec-deployment/#knownpotential-issues-related-to-ansible-playbook-execution","text":"","title":"Known/Potential issues related to Ansible playbook execution"},{"location":"mec-deployment/#you-are-runninng-the-installer-as-root","text":"This error can be resolved by running the playbook with the current user and not the root user , while having sudo privileges: sudo -u $USER ansible-playbook EdgeDeployment.yaml --ask-become-pass","title":"You are runninng the installer as root"},{"location":"mec-deployment/#couldnt-resolve-moduleaction-communitygeneralipinfoio_facts","text":"This error is due to the usage of an old version of ansible, and can be easily resolved through: sudo apt purge ansible sudo apt-add-repository ppa:ansible/ansible sudo apt update sudo apt install ansible ansible-galaxy collection install community.general community.docker kubernetes.core --force","title":"Couldn't resolve module/action 'community.general.ipinfoio_facts'"},{"location":"mec-deployment/#cannot-uninstall-pyyaml","text":"PyYAML is potentially installed and already bundled into some core packages of the distro. To force installation of required PyYAML version: sudo pip install --ignore-installed PyYAML","title":"Cannot uninstall 'PyYAML'"},{"location":"mec-deployment/#knownpotential-issues-related-to-docker","text":"","title":"Known/Potential issues related to Docker"},{"location":"mec-deployment/#docker-credential-secretservice-failure","text":"If the playbook fails in Docker login task (e.g Docker-credential-secretservice fails on Ubuntu 18.04: error getting credentials - err: exit status 1, out:GDBus.Error:org.freedesktop.DBus.Error.ServiceUnknown: The name org.freedesktop.secrets was not provided by any .service files :), run the following commannd and then the ansible again: sudo apt install gnupg2 passls -la","title":"Docker-credential-secretservice failure"},{"location":"mec-deployment/#knownpotential-issues-related-to-kubernetes","text":"","title":"Known/Potential issues related to Kubernetes"},{"location":"mec-deployment/#failure-of-kubernetes-cluster-init-due-to-unvalidated-docker-version","text":"This error is due to the usage of an unvalidated docker version. To fix it add the following arguments like follows: # Not working - kubeadm init --config {{ ansible_env.HOME }}/5gmeta/tmp/cluster-config.yaml > {{ ansible_env.HOME }}/5gmeta/logs/cluster_init # Working - kubeadm init --ignore-preflight-errors=SystemVerification --config {{ ansible_env.HOME }}/5gmeta/tmp/cluster-config.yaml > {{ ansible_env.HOME }}/5gmeta/logs/cluster_init","title":"Failure of Kubernetes Cluster init due to unvalidated docker version"},{"location":"mec-deployment/#failure-with-validating-the-existence-and-emptiness-of-directory-varlibetcd","text":"This error can be easily fixed by removing the directory: sudo rm /var/lib/etcd","title":"Failure with validating the existence and emptiness of directory /var/lib/etcd"},{"location":"mec-deployment/#knownpotential-issues-related-to-osm","text":"","title":"Known/Potential issues related to OSM"},{"location":"mec-deployment/#failure-of-osm-install-with-several-deployements-still-pending","text":"If OSM installation fails due to pending deployements, like below: 5 of 9 deployments starting: lcm 0/1 0 mon 0/1 0 nbi 0/1 0 pol 0/1 0 ro 0/1 0 SYSTEM IS BROKEN OSM is not healthy, but will probably converge to a healthy state soon. Check OSM status with: kubectl -n osm get all DONE Identifying the root cause can be difficult. Still a potential solution is to check the logs related to such deployements like below: sudo kubectl get all -n osm #command output!!! NAME READY STATUS RESTARTS AGE pod/grafana-855d96c47d-w4xfs 2/2 Running 0 66m pod/kafka-0 1/1 Running 3 66m pod/keystone-7d9864f8f5-qx4dd 1/1 Running 0 66m pod/lcm-556b46f977-c2ljl 0/1 Init:0/1 0 66m pod/modeloperator-946f64449-mffbc 1/1 Running 0 66m pod/mon-577c8fc975-9kndp 0/1 Init:0/1 0 66m pod/mysql-0 1/1 Running 0 66m pod/nbi-7886459c9f-mldhw 0/1 Init:0/1 0 66m pod/ng-ui-7d98c6568f-d8862 1/1 Running 0 66m pod/pol-7f86867d95-vm762 0/1 Init:0/1 0 66m pod/prometheus-0 1/1 Running 0 66m pod/ro-ddbc9f888-6jmlp 0/1 Init:0/1 0 66m pod/zookeeper-0 1/1 Running 0 66m Then it is possible to get details related to a specific deployment: kubectl describe pod/lcm-556b46f977-c2ljl -n osm #command output!!! Name: lcm-556b46f977-c2ljl Namespace: osm Priority: 0 .... .... Init Containers: kafka-ro-mongo-test: Container ID: docker://8c4fb0ce225af5c4419441c21b1493da252441b83fbe963d4ef7125c44389591 Image: alpine:latest Image ID: docker-pullable:// .... .... And finally to check its logs and identify the root cause kubectl logs pod/lcm-556b46f977-c2ljl-n osm -c kafka-ro-mongo-test #command output!!! nc: bad address 'mongodb-k8s' kafka (10.244.0.25:9092) open nc: bad address 'mongodb-k8s' kafka (10.244.0.25:9092) open","title":"Failure of OSM install with several deployements still pending"},{"location":"mec-deployment/#osm-command-returning-modulenotfounderror-no-module-named-prettytable","text":"This error can be easily fixed by: sudo apt install python3-prettytable","title":"OSM command returning \"ModuleNotFoundError: No module named 'prettytable'"},{"location":"mec-deployment/#osm-command-returning-typeerror-init-got-an-unexpected-keyword-argument-case_sensitive","text":"This error can be easily fixed by: pip3 install -U click","title":"OSM command returning TypeError: init() got an unexpected keyword argument 'case_sensitive'"},{"location":"platform-walkthrough/","text":"5GMETA Cloud Architecture Features: 2 worker nodes (1 min / 4 max) T3a.large instances (2vCPUs 8GiB Memory) eu-west-3 region (Paris) An Amazon EKS cluster consists of two primary components: The Amazon EKS control plane Amazon EKS nodes that are registered with the control plane The Amazon EKS control plane consists of control plane nodes that run the - Kubernetes software, such as etcd and the Kubernetes API server. The control plane runs in an account managed by AWS, and the Kubernetes API is exposed via the Amazon EKS endpoint associated with your cluster. Prerequisites AWS CLI: https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html Kubectl: https://kubernetes.io/docs/tasks/tools/#kubectl Helm: https://helm.sh/docs/intro/install/ AWS EKS Masterclass: https://github.com/stacksimplify/aws-eks-kubernetes-masterclass , https://www.stacksimplify.com/aws-eks/ Platform access Access request: https://vicomtech.sharepoint.com/ \u274c /s/5GMETA2/EWkBlYytyP5AvA1A-tbzs3MBhRXfhsrLLddMc6c0mpgrrA?e=TugssA AWS CLI credentials will be provided by VICOM: aws configure Using AWS CLI and provided credentials generate kubeconfig file with following command: aws eks update-kubeconfig --name 5gmeta-cloud --role-arn arn:aws:iam::<idaws>:role/k8s5gmeta To change default namespace: kubectl config set-context --current --namespace=<my-namespace> The config file will be created in kubectl default path: ~/.kube/config Kubecofig enables kubectl to securely access the Kubernetes Cluster Kubeconfig is a YAML file that contains either a username and password combination or a secure token that when read programmatically removes the need for the Kubernetes client to ask for interactive authentication Modules deployment in the cloud platform Deployment of apps: Helm charts or K8s manifest files: - Manifest files: - https://kubernetes.io/docs/tutorials/ - https://www.mirantis.com/blog/introduction-to-yaml-creating-a-kubernetes-deployment/ - https://docs.aws.amazon.com/eks/latest/userguide/sample-deployment.html - Helm Charts: - https://helm.sh/docs/chart_template_guide/getting_started/ - https://docs.bitnami.com/tutorials/create-your-first-helm-chart/ - https://veducate.co.uk/how-to-create-helm-chart/ - https://jfrog.com/blog/10-helm-tutorials-to-start-your-kubernetes-journey/ All resources to be deployed in the cloud MUST BE in the project\u2019s offical repositories: - DockerHub: https://hub.docker.com/orgs/5gmeta - Helm Chart repository: https://github.com/5gmeta/helmcharts - GitHub (Manifest files in each module repository): Communication between pods By default, pods can communicate with each other by their IP address, regardless of the namespace they're in. You can see the IP address of each pod with: kubectl get pods -o wide \u2013n Decision taken on limits of connectivity across pods All pods are \"in a LAN\u201c No expected access from third-parties to internal IPs ( https://projectcalico.docs.tigera.io/security/calico-network-policy ) However, the normal way to communicate within a cluster is through Service resources. A Service also has an IP address and additionally a DNS name. A Service is backed by a set of pods. The Service forwards requests to itself to one of the backing pods. The fully qualified DNS name of a Service is: <service-name>.<service-namespace>.svc.cluster.local This can be resolved to the IP address of the Service from anywhere in the cluster (regardless of namespace). https://kubebyexample.com/en/learning-paths/application-development-kubernetes/lesson-3-networking-kubernetes/exposing Persistent storage Amazon EBS CSI driver already installed -> Default storage class Every Persistent Volume Claim (PVC) will create an AWS EBS Volume Volume Integrity is ensured More: https://kubernetes.io/docs/concepts/storage/dynamic-provisioning/ https://loft.sh/blog/kubernetes-persistent-volumes-examples-and-best-practices/ https://github.com/kubernetes-sigs/aws-ebs-csi-driver/tree/master/examples/kubernetes Services exposure to the outside world NodePort: Declaring a service as NodePort exposes the Service on each Node\u2019s IP at the NodePort (a fixed port for that Service, in the default range of 30000-32767). You can then access the Service from outside the cluster by requesting : . Every service you deploy as NodePort will be exposed in its own port, on every Node. LoadBalancer: Declaring a service of type LoadBalancer exposes it externally using a cloud provider\u2019s load balancer. The cloud provider will provision a load balancer for the Service and map it to its automatically assigned NodePort. NLB is created in AWS for each service. Ingress: Ingress exposes HTTP and HTTPS routes from outside the cluster to services within the cluster. Traffic routing is controlled by rules defined on the Ingress resource. ALB is created in AWS. One for all ingress resources. More: https://blog.ovhcloud.com/getting-external-traffic-into-kubernetes-clusterip-nodeport-loadbalancer-and-ingress/ https://alesnosek.com/blog/2017/02/14/accessing-kubernetes-pods-from-outside-of-the-cluster/ https://www.eksworkshop.com/beginner/130_exposing-service/ingress/ Monitoring of modules Prometheus and Grafana already deployed: PROMETHEUS=$(kubectl get pod -n monitoring -l app.kubernetes.io/name=prometheus -o jsonpath='{.items[0].metadata.name}') GRAFANA=$(kubectl get pod -n monitoring -l app.kubernetes.io/name=grafana -o jsonpath='{.items[0].metadata.name}') kubectl port-forward -n monitoring $PROMETHEUS 9090 & kubectl port-forward -n monitoring $GRAFANA 3000 & Pipeline development and deployment Flow for deploying a pipeline in an Edge Server: Receive a request from the cloud platform Check if enough resources available in the node for the selected SLA If enough, save the reservation in the DDBB Instantiate the pipeline through OSM Development of a pipeline: Docker images: https://hub.docker.com/orgs/5gmeta/repositories Helm chart to deploy in K8s: https://github.com/5gmeta/helmcharts OSM descriptors to deploy the Helm chart: https://github.com/5gmeta/vnfdescriptors","title":"5GMETA Cloud Architecture"},{"location":"platform-walkthrough/#5gmeta-cloud-architecture","text":"","title":"5GMETA Cloud Architecture"},{"location":"platform-walkthrough/#features","text":"2 worker nodes (1 min / 4 max) T3a.large instances (2vCPUs 8GiB Memory) eu-west-3 region (Paris) An Amazon EKS cluster consists of two primary components: The Amazon EKS control plane Amazon EKS nodes that are registered with the control plane The Amazon EKS control plane consists of control plane nodes that run the - Kubernetes software, such as etcd and the Kubernetes API server. The control plane runs in an account managed by AWS, and the Kubernetes API is exposed via the Amazon EKS endpoint associated with your cluster.","title":"Features:"},{"location":"platform-walkthrough/#prerequisites","text":"AWS CLI: https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html Kubectl: https://kubernetes.io/docs/tasks/tools/#kubectl Helm: https://helm.sh/docs/intro/install/ AWS EKS Masterclass: https://github.com/stacksimplify/aws-eks-kubernetes-masterclass , https://www.stacksimplify.com/aws-eks/","title":"Prerequisites"},{"location":"platform-walkthrough/#platform-access","text":"Access request: https://vicomtech.sharepoint.com/ \u274c /s/5GMETA2/EWkBlYytyP5AvA1A-tbzs3MBhRXfhsrLLddMc6c0mpgrrA?e=TugssA AWS CLI credentials will be provided by VICOM: aws configure Using AWS CLI and provided credentials generate kubeconfig file with following command: aws eks update-kubeconfig --name 5gmeta-cloud --role-arn arn:aws:iam::<idaws>:role/k8s5gmeta To change default namespace: kubectl config set-context --current --namespace=<my-namespace> The config file will be created in kubectl default path: ~/.kube/config Kubecofig enables kubectl to securely access the Kubernetes Cluster Kubeconfig is a YAML file that contains either a username and password combination or a secure token that when read programmatically removes the need for the Kubernetes client to ask for interactive authentication","title":"Platform  access"},{"location":"platform-walkthrough/#modules-deployment-in-the-cloud-platform","text":"Deployment of apps: Helm charts or K8s manifest files: - Manifest files: - https://kubernetes.io/docs/tutorials/ - https://www.mirantis.com/blog/introduction-to-yaml-creating-a-kubernetes-deployment/ - https://docs.aws.amazon.com/eks/latest/userguide/sample-deployment.html - Helm Charts: - https://helm.sh/docs/chart_template_guide/getting_started/ - https://docs.bitnami.com/tutorials/create-your-first-helm-chart/ - https://veducate.co.uk/how-to-create-helm-chart/ - https://jfrog.com/blog/10-helm-tutorials-to-start-your-kubernetes-journey/ All resources to be deployed in the cloud MUST BE in the project\u2019s offical repositories: - DockerHub: https://hub.docker.com/orgs/5gmeta - Helm Chart repository: https://github.com/5gmeta/helmcharts - GitHub (Manifest files in each module repository):","title":"Modules deployment in the cloud platform"},{"location":"platform-walkthrough/#communication-between-pods","text":"By default, pods can communicate with each other by their IP address, regardless of the namespace they're in. You can see the IP address of each pod with: kubectl get pods -o wide \u2013n Decision taken on limits of connectivity across pods All pods are \"in a LAN\u201c No expected access from third-parties to internal IPs ( https://projectcalico.docs.tigera.io/security/calico-network-policy ) However, the normal way to communicate within a cluster is through Service resources. A Service also has an IP address and additionally a DNS name. A Service is backed by a set of pods. The Service forwards requests to itself to one of the backing pods. The fully qualified DNS name of a Service is: <service-name>.<service-namespace>.svc.cluster.local This can be resolved to the IP address of the Service from anywhere in the cluster (regardless of namespace). https://kubebyexample.com/en/learning-paths/application-development-kubernetes/lesson-3-networking-kubernetes/exposing","title":"Communication  between  pods"},{"location":"platform-walkthrough/#persistent-storage","text":"Amazon EBS CSI driver already installed -> Default storage class Every Persistent Volume Claim (PVC) will create an AWS EBS Volume Volume Integrity is ensured More: https://kubernetes.io/docs/concepts/storage/dynamic-provisioning/ https://loft.sh/blog/kubernetes-persistent-volumes-examples-and-best-practices/ https://github.com/kubernetes-sigs/aws-ebs-csi-driver/tree/master/examples/kubernetes","title":"Persistent storage"},{"location":"platform-walkthrough/#services-exposure-to-the-outside-world","text":"NodePort: Declaring a service as NodePort exposes the Service on each Node\u2019s IP at the NodePort (a fixed port for that Service, in the default range of 30000-32767). You can then access the Service from outside the cluster by requesting : . Every service you deploy as NodePort will be exposed in its own port, on every Node. LoadBalancer: Declaring a service of type LoadBalancer exposes it externally using a cloud provider\u2019s load balancer. The cloud provider will provision a load balancer for the Service and map it to its automatically assigned NodePort. NLB is created in AWS for each service. Ingress: Ingress exposes HTTP and HTTPS routes from outside the cluster to services within the cluster. Traffic routing is controlled by rules defined on the Ingress resource. ALB is created in AWS. One for all ingress resources. More: https://blog.ovhcloud.com/getting-external-traffic-into-kubernetes-clusterip-nodeport-loadbalancer-and-ingress/ https://alesnosek.com/blog/2017/02/14/accessing-kubernetes-pods-from-outside-of-the-cluster/ https://www.eksworkshop.com/beginner/130_exposing-service/ingress/","title":"Services exposure to the outside world"},{"location":"platform-walkthrough/#monitoring-of-modules","text":"Prometheus and Grafana already deployed: PROMETHEUS=$(kubectl get pod -n monitoring -l app.kubernetes.io/name=prometheus -o jsonpath='{.items[0].metadata.name}') GRAFANA=$(kubectl get pod -n monitoring -l app.kubernetes.io/name=grafana -o jsonpath='{.items[0].metadata.name}') kubectl port-forward -n monitoring $PROMETHEUS 9090 & kubectl port-forward -n monitoring $GRAFANA 3000 &","title":"Monitoring  of modules"},{"location":"platform-walkthrough/#pipeline-development-and-deployment","text":"Flow for deploying a pipeline in an Edge Server: Receive a request from the cloud platform Check if enough resources available in the node for the selected SLA If enough, save the reservation in the DDBB Instantiate the pipeline through OSM Development of a pipeline: Docker images: https://hub.docker.com/orgs/5gmeta/repositories Helm chart to deploy in K8s: https://github.com/5gmeta/helmcharts OSM descriptors to deploy the Helm chart: https://github.com/5gmeta/vnfdescriptors","title":"Pipeline development and deployment"},{"location":"smtp-server/","text":"SMTP Server Amazon Simple Email Service (SES) is a cloud-based email service that provides cost-effective, flexible and scalable way to keep in contact with their customers through email. SMTP-enabled programming language, email server, or application can be used to connect to the Amazon SES SMTP interface. The following information and a set of SMTP credentials is needed to configure this email sending method. SMTP endpoint: email-smtp.eu-west-3.amazonaws.com Transport Layer Security (TLS): Required STARTTLS Port: 25 , 587 or 2587 TLS Wrapper Port: 465 or 2465 Some credentials have been already configured for the project. It can be found in the project's repository: credentials.csv . In AWS the user created is: ses-smtp-user.20221028-131744 More details about the SMTP implementation for 5GMETA. All the mails sent by the platform must use the following address: no-reply@5gmeta-platform.eu There are no mailboxes for receiving e-mails Need to authenticate into the server. Every partner should fill the credentials request .","title":"SMTP Server"},{"location":"smtp-server/#smtp-server","text":"Amazon Simple Email Service (SES) is a cloud-based email service that provides cost-effective, flexible and scalable way to keep in contact with their customers through email. SMTP-enabled programming language, email server, or application can be used to connect to the Amazon SES SMTP interface. The following information and a set of SMTP credentials is needed to configure this email sending method. SMTP endpoint: email-smtp.eu-west-3.amazonaws.com Transport Layer Security (TLS): Required STARTTLS Port: 25 , 587 or 2587 TLS Wrapper Port: 465 or 2465 Some credentials have been already configured for the project. It can be found in the project's repository: credentials.csv . In AWS the user created is: ses-smtp-user.20221028-131744 More details about the SMTP implementation for 5GMETA. All the mails sent by the platform must use the following address: no-reply@5gmeta-platform.eu There are no mailboxes for receiving e-mails Need to authenticate into the server. Every partner should fill the credentials request .","title":"SMTP Server"},{"location":"troubleshooting/","text":"Platform reboot If k8s nodes are rebooted, some ips must be modified Get new public ips from k8s nodes Go to: https://eu-west-3.console.aws.amazon.com/eks/home?region=eu-west-3#/clusters/5gmeta-cloud?selectedTab=cluster-compute-tab Go to console Amazon web page -> EKS -> click on 5gmeta-cloud -> compute tab Open each compute and click on instance. There you can find public IP Add ips to security groups in Amazon EKS Add new ips in pl-0f53017bf8a4dc952 prefix list: https://eu-west-3.console.aws.amazon.com/vpc/home?region=eu-west-3#PrefixListDetails:PrefixListId=pl-0f53017bf8a4dc952 Check that pl-0f53017bf8a4dc952 prefix list is allowed to access 61616 TCP port in security group sg-07e47c7047324400b https://eu-west-3.console.aws.amazon.com/vpc/home?region=eu-west-3#SecurityGroup:groupId=sg-07e47c7047324400b Reconfigure Kafka cluster Add new ips in [https://github.com/5gmeta/stream-data-gateway/blob/main/src/prod-version/cp-helm-charts/charts/cp-kafka/values.yaml] And reinstall helm chart: helm get values kafkacluster -n kafka helm uninstall kafkacluster -n kafka kubectl delete namespace kafka from stream-data-gateway/src/prod-version git folder helm install kafkacluster ./cp-helm-charts --create-namespace -n kafka Reconfigure Registration API Update access to 5GMETA database: Add one of k8s nodes new ip address in https://github.com/5gmeta/helmcharts/blob/main/charts/registrationapi-chart/values.yaml Rebuild registration API: - helm package ./charts/* -d ./repository/ - helm repo index ./repository/ - Push the changes to the repository Ask parters to update Helm charts from their MECS: Log in into your MEC Update 5GMETA helm repositories helm repo update 5gmeta Uninstall previous registration-api helm helm uninstall registration-api -n 5gmeta Install updated registration-api helm helm install registration-api 5gmeta/registrationapi-chart -n 5gmeta Update all examples and Add load balancer ips to allowed ips Ping to 5gmeta-platform.eu or to DNS hostname (get from https://eu-west-3.console.aws.amazon.com/ec2/home?region=eu-west-3#LoadBalancers:). It will return 3 ips. Copy them into pl-0f53017bf8a4dc952 prefix list: https://eu-west-3.console.aws.amazon.com/vpc/home?region=eu-west-3#PrefixListDetails:PrefixListId=pl-0f53017bf8a4dc952 Reboot Load balancer (APISIX/ GATEWAY) If new whitelisted ip addresses are needed, a new prefix list ( https://eu-west-3.console.aws.amazon.com/vpc/home?region=eu-west-3#ManagedPrefixLists: ) with them should be filled. That list must have access to the ports 80 (HTTP) and 443 (HTTPS). To do that we have to create a new security group that points to new prefix list. Best option is to copy https://eu-west-3.console.aws.amazon.com/vpc/home?region=eu-west-3#SecurityGroup:groupId=sg-06755943a0511d718 to a new rule and add prefix list as source ip addresses. Add that created security to [https://github.com/5gmeta/helmcharts/blob/main/charts/gateway-chart/values.yaml] and rebuild helm chart and uninstall/install from EKS. alb.ingress.kubernetes.io/security-groups: sg-06755943a0511d718,new_security_group Rebuild Helm charts helm package ./charts/* -d ./repository/ helm repo index ./repository/ Push the changes to the repository Uninstall / Install Gateway helm uninstall apisix-service -n akka helm install apisix-service 5gmeta-helm/gateway-standalone -n akka Update 5gmeta-platform.eu hostname When a new Load balancer is deployed, DNS hostname is modified, so it must be changed in Amazon DNS manager: To check Load balancer hostname go to [ https://eu-west-3.console.aws.amazon.com/ec2/home?region=eu-west-3#LoadBalancers: and replace it in https://us-east-1.console.aws.amazon.com/route53/v2/home#Dashboard . Edit hosted zones and modify A record from 5gmeta-platform.eu adding DNS hosted zone +dualstack: dualstack.k8s-5gmetaingress-253cf23a0c-1635503379.eu-west-3.elb.amazonaws.com for example.","title":"Platform reboot"},{"location":"troubleshooting/#platform-reboot","text":"If k8s nodes are rebooted, some ips must be modified","title":"Platform reboot"},{"location":"troubleshooting/#get-new-public-ips-from-k8s-nodes","text":"Go to: https://eu-west-3.console.aws.amazon.com/eks/home?region=eu-west-3#/clusters/5gmeta-cloud?selectedTab=cluster-compute-tab Go to console Amazon web page -> EKS -> click on 5gmeta-cloud -> compute tab Open each compute and click on instance. There you can find public IP","title":"Get new public ips from k8s nodes"},{"location":"troubleshooting/#add-ips-to-security-groups-in-amazon-eks","text":"Add new ips in pl-0f53017bf8a4dc952 prefix list: https://eu-west-3.console.aws.amazon.com/vpc/home?region=eu-west-3#PrefixListDetails:PrefixListId=pl-0f53017bf8a4dc952 Check that pl-0f53017bf8a4dc952 prefix list is allowed to access 61616 TCP port in security group sg-07e47c7047324400b https://eu-west-3.console.aws.amazon.com/vpc/home?region=eu-west-3#SecurityGroup:groupId=sg-07e47c7047324400b","title":"Add ips to security groups in Amazon EKS"},{"location":"troubleshooting/#reconfigure-kafka-cluster","text":"Add new ips in [https://github.com/5gmeta/stream-data-gateway/blob/main/src/prod-version/cp-helm-charts/charts/cp-kafka/values.yaml] And reinstall helm chart: helm get values kafkacluster -n kafka helm uninstall kafkacluster -n kafka kubectl delete namespace kafka from stream-data-gateway/src/prod-version git folder helm install kafkacluster ./cp-helm-charts --create-namespace -n kafka","title":"Reconfigure Kafka cluster"},{"location":"troubleshooting/#reconfigure-registration-api","text":"Update access to 5GMETA database: Add one of k8s nodes new ip address in https://github.com/5gmeta/helmcharts/blob/main/charts/registrationapi-chart/values.yaml Rebuild registration API: - helm package ./charts/* -d ./repository/ - helm repo index ./repository/ - Push the changes to the repository Ask parters to update Helm charts from their MECS: Log in into your MEC Update 5GMETA helm repositories helm repo update 5gmeta Uninstall previous registration-api helm helm uninstall registration-api -n 5gmeta Install updated registration-api helm helm install registration-api 5gmeta/registrationapi-chart -n 5gmeta","title":"Reconfigure Registration API"},{"location":"troubleshooting/#update-all-examples-and","text":"","title":"Update all examples and"},{"location":"troubleshooting/#add-load-balancer-ips-to-allowed-ips","text":"Ping to 5gmeta-platform.eu or to DNS hostname (get from https://eu-west-3.console.aws.amazon.com/ec2/home?region=eu-west-3#LoadBalancers:). It will return 3 ips. Copy them into pl-0f53017bf8a4dc952 prefix list: https://eu-west-3.console.aws.amazon.com/vpc/home?region=eu-west-3#PrefixListDetails:PrefixListId=pl-0f53017bf8a4dc952","title":"Add load balancer ips to allowed ips"},{"location":"troubleshooting/#reboot-load-balancer-apisix-gateway","text":"If new whitelisted ip addresses are needed, a new prefix list ( https://eu-west-3.console.aws.amazon.com/vpc/home?region=eu-west-3#ManagedPrefixLists: ) with them should be filled. That list must have access to the ports 80 (HTTP) and 443 (HTTPS). To do that we have to create a new security group that points to new prefix list. Best option is to copy https://eu-west-3.console.aws.amazon.com/vpc/home?region=eu-west-3#SecurityGroup:groupId=sg-06755943a0511d718 to a new rule and add prefix list as source ip addresses. Add that created security to [https://github.com/5gmeta/helmcharts/blob/main/charts/gateway-chart/values.yaml] and rebuild helm chart and uninstall/install from EKS. alb.ingress.kubernetes.io/security-groups: sg-06755943a0511d718,new_security_group","title":"Reboot Load balancer (APISIX/ GATEWAY)"},{"location":"troubleshooting/#rebuild-helm-charts","text":"helm package ./charts/* -d ./repository/ helm repo index ./repository/ Push the changes to the repository","title":"Rebuild Helm charts"},{"location":"troubleshooting/#uninstall-install-gateway","text":"helm uninstall apisix-service -n akka helm install apisix-service 5gmeta-helm/gateway-standalone -n akka","title":"Uninstall / Install Gateway"},{"location":"troubleshooting/#update-5gmeta-platformeu-hostname","text":"When a new Load balancer is deployed, DNS hostname is modified, so it must be changed in Amazon DNS manager: To check Load balancer hostname go to [ https://eu-west-3.console.aws.amazon.com/ec2/home?region=eu-west-3#LoadBalancers: and replace it in https://us-east-1.console.aws.amazon.com/route53/v2/home#Dashboard . Edit hosted zones and modify A record from 5gmeta-platform.eu adding DNS hosted zone +dualstack: dualstack.k8s-5gmetaingress-253cf23a0c-1635503379.eu-west-3.elb.amazonaws.com for example.","title":"Update 5gmeta-platform.eu hostname"}]}